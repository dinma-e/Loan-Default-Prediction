{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinma-e/Loan-Default-Prediction/blob/main/Predicting_Loan_Default_Risk_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc510fe5-e07f-4cb1-9731-3b672da08abb",
      "metadata": {
        "id": "bc510fe5-e07f-4cb1-9731-3b672da08abb"
      },
      "source": [
        "# Introduction\n",
        "Financial institutions face a constant challenge: balancing the need to provide loans with the risk of borrowers defaulting. Every default not only leads to financial loss but also affects trust and long-term sustainability. With the rise of digital lending platforms, the ability to anticipate potential defaults has become more important than ever.\n",
        "\n",
        "As part of my data science journey, I worked on this project as a capstone to apply machine learning techniques to the problem of loan default prediction. The goal was not just to see how well models could classify borrowers, but also to uncover meaningful insights from the data. Rather than focusing only on accuracy, I aimed to understand the dataset, build predictive models, evaluate their performance, and reflect on the lessons learned.\n",
        "\n",
        "# Problem Statement\n",
        "Develop a robust machine learning pipeline to predict loan default risk, enabling better credit decisions and minimizing financial losses.\n",
        "\n",
        "# Objectives\n",
        "The main objectives of this project are:\n",
        "1. Data Understanding, Analysis & Preparation: Explore the data sets, gain insights and preprocess the data for model development (handle missing values, encode categorical variables, and scale features).\n",
        "2. Model Development: Build and test machine learning models to classify borrowers as likely defaulters or non defaulters.\n",
        "3. Evaluation & Insights: Assess performance using metrics such as recall, precision, and F1-score, with a focus on identifying high risk borrowers.\n",
        "4. Lessons & Recommendations: Reflect on the challenges faced and share insights that could inform future projects or real world applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196c0d9c-6c63-4f19-a673-f6de3c4a4239",
      "metadata": {
        "id": "196c0d9c-6c63-4f19-a673-f6de3c4a4239"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import geopandas as gpd\n",
        "import shap\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE, RFECV\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, make_scorer, recall_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d226ae4b-1033-43a5-adaa-2229c49e67b4",
      "metadata": {
        "id": "d226ae4b-1033-43a5-adaa-2229c49e67b4"
      },
      "outputs": [],
      "source": [
        "# data collection - get the data sets\n",
        "url1 = 'https://raw.githubusercontent.com/Oyeniran20/axia_cohort_8/refs/heads/main/trainperf.csv'\n",
        "url2 = 'https://raw.githubusercontent.com/Oyeniran20/axia_cohort_8/refs/heads/main/traindemographics.csv'\n",
        "url3 = 'https://raw.githubusercontent.com/Oyeniran20/axia_cohort_8/refs/heads/main/trainprevloans.csv'\n",
        "\n",
        "# convert to data frames\n",
        "df_demographics = pd.read_csv(url2)\n",
        "df_perf = pd.read_csv(url1)\n",
        "df_prevloans = pd.read_csv(url3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98a9a2ae-c658-454e-baea-630996695541",
      "metadata": {
        "id": "98a9a2ae-c658-454e-baea-630996695541"
      },
      "source": [
        "## Data Understanding and Cleaning\n",
        "- Examine each dataframe to understand its purpose and content\n",
        "- Perform basic cleaning like removing duplicates, dropping columns with a high percentage of missing values, changing data types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df341125-3b4f-45aa-9396-f40399dae335",
      "metadata": {
        "id": "df341125-3b4f-45aa-9396-f40399dae335"
      },
      "source": [
        "#### Column meanings\n",
        "|Column|Meaning|\n",
        "|------|-------|\n",
        "|customerid|A unique identifier for each customer|\n",
        "|systemloanid|A unique identifier for each loan|\n",
        "|loannumber|A count of how many loans a customer has taken|\n",
        "|approveddate|The date the loan was approved|\n",
        "|creationdate|The date the loan application was submitted|\n",
        "|loanamount|The amount approved and loaned to the customer (principal amount)|\n",
        "|totaldue|The amount to be paid back by the customer (principal + interest)|\n",
        "|termdays|The loan duration in days|\n",
        "|good_bad_flag|Indicates if the customer defaulted or not (Target column)|\n",
        "|birthdate|The date of birth of the customer|\n",
        "|bank_account_type|The type of bank account the customer has|\n",
        "|longitude_gps|The customer's location longitude|\n",
        "|latitude_gps|The customer's location latitude|\n",
        "|bank_name_clients|The name of the customer's bank|\n",
        "|employment_status_clients|The employment status of the customer|\n",
        "|level_of_education_clients|The highest level of education of the customer|\n",
        "|closeddate|The date the loan was officially closed|\n",
        "|firstduedate|The first due date of repayment|\n",
        "|firstrepaiddate|The date of the first repayment made by the customer|"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d20645-4a3d-4c7b-aa81-7e571cf552e6",
      "metadata": {
        "id": "53d20645-4a3d-4c7b-aa81-7e571cf552e6"
      },
      "source": [
        "### Data Understanding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e369ab2-c174-4e8f-93bc-121f3437d24a",
      "metadata": {
        "id": "4e369ab2-c174-4e8f-93bc-121f3437d24a"
      },
      "outputs": [],
      "source": [
        "# performance dataframe\n",
        "df_perf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47de248-3df4-4417-8aa5-e625c16f6581",
      "metadata": {
        "id": "b47de248-3df4-4417-8aa5-e625c16f6581"
      },
      "outputs": [],
      "source": [
        "# previous loans dataframe\n",
        "df_prevloans.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2319e81c-ba34-46bd-9145-96b4d2f6cfca",
      "metadata": {
        "id": "2319e81c-ba34-46bd-9145-96b4d2f6cfca"
      },
      "outputs": [],
      "source": [
        "# demographics dataframe\n",
        "df_demographics.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d51c9c-2652-4ee0-9e05-a53db603ae5e",
      "metadata": {
        "id": "f1d51c9c-2652-4ee0-9e05-a53db603ae5e"
      },
      "outputs": [],
      "source": [
        "# check for common points in dataframes - using customerid 8a2a81a74ce8c05d014cfb32a0da1049"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438580a9-6bec-4b7a-9814-b5909dbf4c49",
      "metadata": {
        "id": "438580a9-6bec-4b7a-9814-b5909dbf4c49"
      },
      "outputs": [],
      "source": [
        "# performance\n",
        "df_perf[df_perf['customerid'] == '8a2a81a74ce8c05d014cfb32a0da1049']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0183c99-769d-4005-a996-0c8d8fec8ae7",
      "metadata": {
        "id": "d0183c99-769d-4005-a996-0c8d8fec8ae7"
      },
      "outputs": [],
      "source": [
        "# demographics\n",
        "df_demographics[df_demographics['customerid'] == '8a2a81a74ce8c05d014cfb32a0da1049']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a7d4fa-47db-41cd-aa19-c82cd84a17c5",
      "metadata": {
        "id": "48a7d4fa-47db-41cd-aa19-c82cd84a17c5"
      },
      "outputs": [],
      "source": [
        "# previous loans\n",
        "df_prevloans[df_prevloans['customerid'] == '8a2a81a74ce8c05d014cfb32a0da1049']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a694c9a-7814-4288-ad93-5ef0d359ee12",
      "metadata": {
        "id": "0a694c9a-7814-4288-ad93-5ef0d359ee12"
      },
      "outputs": [],
      "source": [
        "# create a dictionary containing the data frames for examination purposes\n",
        "dataframes = {\n",
        "    'df_demographics' : df_demographics,\n",
        "    'df_perf' : df_perf,\n",
        "    'df_prevloans' : df_prevloans}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c4f715-2a76-4893-83b6-b72d478e7e82",
      "metadata": {
        "id": "27c4f715-2a76-4893-83b6-b72d478e7e82"
      },
      "source": [
        "#### Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e60d05b-07bf-4e75-91db-4ba0198803d9",
      "metadata": {
        "id": "6e60d05b-07bf-4e75-91db-4ba0198803d9"
      },
      "outputs": [],
      "source": [
        "for name, data in dataframes.items():\n",
        "    print(f\"Shape of {name} = {data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cdbe502-4d00-4cd0-afdf-51ed29300a34",
      "metadata": {
        "id": "9cdbe502-4d00-4cd0-afdf-51ed29300a34"
      },
      "source": [
        "#### Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d60d32-71ac-47d6-97f6-6919dad993df",
      "metadata": {
        "id": "b7d60d32-71ac-47d6-97f6-6919dad993df"
      },
      "outputs": [],
      "source": [
        "# general\n",
        "for name, data in dataframes.items():\n",
        "    print(f\"Number of duplicates in {name} = {data.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f525d6b0-8432-464f-8b8f-7954319d354c",
      "metadata": {
        "id": "f525d6b0-8432-464f-8b8f-7954319d354c"
      },
      "outputs": [],
      "source": [
        "# customer id\n",
        "for name, data in dataframes.items():\n",
        "    print(f\"Number of duplicates in {name} = {data['customerid'].duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc63e3af-4428-4c29-adfc-a0636720f8a4",
      "metadata": {
        "id": "bc63e3af-4428-4c29-adfc-a0636720f8a4"
      },
      "source": [
        "#### Missing Values (percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6cf69c2-00b8-4c3e-bd59-4fdd33671732",
      "metadata": {
        "id": "a6cf69c2-00b8-4c3e-bd59-4fdd33671732"
      },
      "outputs": [],
      "source": [
        "for name, data in dataframes.items():\n",
        "    print(f\"Percentage of missing values in {name}:\")\n",
        "    print(f\"{(data.isna().sum().sort_values(ascending=False)/len(data))*100}\")\n",
        "    print('---' * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8032dafb-914d-4d11-aa69-d36ffddda2fd",
      "metadata": {
        "id": "8032dafb-914d-4d11-aa69-d36ffddda2fd"
      },
      "source": [
        "#### Data Types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95040478-0284-4930-86b4-1d4d1fecca73",
      "metadata": {
        "id": "95040478-0284-4930-86b4-1d4d1fecca73"
      },
      "outputs": [],
      "source": [
        "for name, data in dataframes.items():\n",
        "    print(f\"Data Types of Columns in {name}:\")\n",
        "    print(f\"{data.dtypes}\")\n",
        "    print('---' * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "266d5ef5-22db-4294-b04d-d4da148a68cf",
      "metadata": {
        "id": "266d5ef5-22db-4294-b04d-d4da148a68cf"
      },
      "source": [
        "#### **Observations:**\n",
        "- The performance dataframe (df_perf) contains unique customer loan information and has the column 'good_bad_flag' that tells us whether a customer defaulted or not. This 'good_bad_flag' is the target column\n",
        "- The demographics dataframe is additional information on the customers such as date of birth and employment status\n",
        "- The previous loans dataframe is historical information on loans given to current customers\n",
        "- The previous loans dataframe has a many to one relationship on the customer id. There are 13,824 duplicates of the customerid in df_prevloans\n",
        "- The dataframes have different shapes:\n",
        "  - Shape of df_demographics = 4346 rows, 9 columns\n",
        "  - Shape of df_perf = 4368 rows, 10 columns\n",
        "  - Shape of df_prevloans = 18,183 rows, 12 columns\n",
        "- There are 12 duplicates overall in df_demographics\n",
        "- The bank_branch_clients column in demographics is missing approximately 98.8 percent of values\n",
        "- The level_of_education_clients column in demographics is missing approximately 86.5 percent of values\n",
        "- The employment_status_clients in demographics is missing approximately 14.9 percent of values\n",
        "- The referredby column is missing approximately 86.6 percent and 94.4 percent of values in df_perf and df_prevloans respectively\n",
        "- The date columns are of the wrong data type - object\n",
        "  \n",
        "#### **Insights:**\n",
        "- Some customers are missing demographic information\n",
        "- The demographics dataframe should be merged to the performance dataframe on the customer id because the performance dataframe contains the target column and the customer id is the common point between them.\n",
        "- A single customerid in the previous loans dataframe comes with multiple unique sets of information (the number of general duplicates was observed to be 0) and the columns are not fully in accordance with that of the performance dataframe.\n",
        "- The previous loans dataframe cannot be merged directly due to its duplicates in the customerid.\n",
        "- Although the previous loans dataframe cannot be merged directly with the others, new features that are unique to a customerid can be engineered from it which can then be merged to the main dataframe (performance + demographics). It can also be used for analysis to gain more insights.\n",
        "\n",
        "#### **To Do:**\n",
        "- Change the data type of the date columns to datetime\n",
        "- Drop the duplicates in df_demographics\n",
        "- Drop the referredby, bank_branch_clients columns and level_of_education_clients as there are too many missing values to gain any valuable insights from them"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133ff23b-38a5-4e5d-a6bb-ac0fb1841535",
      "metadata": {
        "id": "133ff23b-38a5-4e5d-a6bb-ac0fb1841535"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d2214c-dd7e-4860-9587-999b1a939088",
      "metadata": {
        "id": "64d2214c-dd7e-4860-9587-999b1a939088"
      },
      "outputs": [],
      "source": [
        "# change date data types\n",
        "date_cols = ['approveddate', 'creationdate', 'birthdate', 'firstrepaiddate', 'firstduedate', 'closeddate']\n",
        "\n",
        "for col in date_cols:\n",
        "    for name, data in dataframes.items():\n",
        "        if col in data.columns:\n",
        "            data[col] = pd.to_datetime(data[col])\n",
        "\n",
        "df_perf = dataframes['df_perf']\n",
        "df_prevloans = dataframes['df_prevloans']\n",
        "df_demographics = dataframes['df_demographics']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0dc9768-51f2-43f8-8c9c-62103bafa321",
      "metadata": {
        "id": "b0dc9768-51f2-43f8-8c9c-62103bafa321"
      },
      "outputs": [],
      "source": [
        "# drop duplicates in demographics\n",
        "df_demographics.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4348efae-4ec3-4e07-9cd7-877c5c3b8ce0",
      "metadata": {
        "id": "4348efae-4ec3-4e07-9cd7-877c5c3b8ce0"
      },
      "outputs": [],
      "source": [
        "# drop columns with high percentage of missing values\n",
        "df_demographics = df_demographics.drop(columns=['bank_branch_clients', 'level_of_education_clients'])\n",
        "df_perf = df_perf.drop(columns='referredby')\n",
        "df_prevloans = df_prevloans.drop(columns='referredby')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8d61f51-599c-4e34-a30d-eba49629dfb4",
      "metadata": {
        "id": "f8d61f51-599c-4e34-a30d-eba49629dfb4"
      },
      "source": [
        "## Feature Engineering and Merging\n",
        "- Engineer new features from the previous loans dataframe\n",
        "- Merge dataframes\n",
        "- Engineer new features from the merged data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd9e69a-ffb8-4675-a78b-59c2c03e8f98",
      "metadata": {
        "id": "3fd9e69a-ffb8-4675-a78b-59c2c03e8f98"
      },
      "source": [
        "### Feature Engineering - Previous Loans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104254f3-3213-4b88-96ff-9bfac76241f4",
      "metadata": {
        "id": "104254f3-3213-4b88-96ff-9bfac76241f4"
      },
      "source": [
        "#### For Merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "444960ff-3c6a-4146-a66a-10c300e19d9a",
      "metadata": {
        "id": "444960ff-3c6a-4146-a66a-10c300e19d9a"
      },
      "outputs": [],
      "source": [
        "# create a function that determines a default/no default on past loans\n",
        "def loan_default(data):\n",
        "    # loan duration\n",
        "    data['loan_duration'] = (data['closeddate'] - data['approveddate']).dt.days\n",
        "\n",
        "    # term delay\n",
        "    data['term_delay'] = data['loan_duration'] - data['termdays']\n",
        "\n",
        "    # no default\n",
        "    data['no_default'] = np.where((data['term_delay'] <= 0), 1, 0)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60959c82-f041-4228-8748-6d273edddef5",
      "metadata": {
        "id": "60959c82-f041-4228-8748-6d273edddef5"
      },
      "outputs": [],
      "source": [
        "# create a fuction that engineers unique features on prevloans\n",
        "def merge_prevloans(data):\n",
        "    # apply customer default function\n",
        "    data = loan_default(data)\n",
        "\n",
        "    # no. of previous defaults/no defaults by customer\n",
        "    customer_default = pd.crosstab(data['customerid'], data['no_default'])\n",
        "    customer_default.columns = ['default_count', 'no_default_count']\n",
        "    data = pd.merge(data, customer_default, how='left', on='customerid')\n",
        "\n",
        "    # previous early and on time repayments\n",
        "    data['early'] = np.where(data['term_delay'] < 0, 1, 0)\n",
        "    data['on_time'] = np.where(data['term_delay'] == 0, 1, 0)\n",
        "\n",
        "    # no. of early/on time payments by customer\n",
        "    early_count = pd.crosstab(data['customerid'], data['early'])\n",
        "    early_count.columns = ['0', 'early_count']\n",
        "    data = pd.merge(data, early_count, how='left', on='customerid')\n",
        "\n",
        "    on_time_count = pd.crosstab(data['customerid'], data['on_time'])\n",
        "    on_time_count.columns = ['1', 'on_time_count']\n",
        "    data = pd.merge(data, on_time_count, how='left', on='customerid')\n",
        "\n",
        "\n",
        "    #previous number of loans taken\n",
        "    prev_number = (data.groupby('customerid')['loannumber'].max()).reset_index(name='prev_loan_number')\n",
        "    data = pd.merge(data, prev_number, how='left', on='customerid')\n",
        "\n",
        "    # previous default rate\n",
        "    data['default_rate'] = (data['default_count']/data['prev_loan_number']).round(2)\n",
        "\n",
        "    # previous total amount of loans taken\n",
        "    prev_total = (data.groupby('customerid')['loanamount'].sum()).reset_index(name='prev_total_loan')\n",
        "    data = data = pd.merge(data, prev_total, how='left', on='customerid')\n",
        "\n",
        "    # previous maximum amount of loans taken\n",
        "    prev_amount = (data.groupby('customerid')['loanamount'].max()).reset_index(name='prev_max_loan_amount')\n",
        "    data = pd.merge(data, prev_amount, how='left', on='customerid')\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8649ffdb-ef4b-4cb1-813e-24e440824d70",
      "metadata": {
        "id": "8649ffdb-ef4b-4cb1-813e-24e440824d70"
      },
      "source": [
        "From the understanding of the previous loans dataframe, the following unique features were created for merging, analysis and possibly prediction:\n",
        "- default_count - Number of times the customer has defaulted\n",
        "- no_default_count - Number of times the customer has not defaulted\n",
        "- early_count - Number of times the customer repaid early\n",
        "- on_time_count - Number of times the customer repaid on time\n",
        "- prev_loan_number - Total number of loans the customer has taken previously\n",
        "- default_rate - The customer's previous default rate (default_count/prev_loan_number)\n",
        "- prev_total_loan - Total amount of the loans the customer had previously taken\n",
        "- prev_max_loan_amount - Maximum amount of a loan the customer had previously taken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "277470d3-dbbe-4477-8af2-ea97ab01cd91",
      "metadata": {
        "id": "277470d3-dbbe-4477-8af2-ea97ab01cd91"
      },
      "outputs": [],
      "source": [
        "# create the dataframe to be used for merging\n",
        "df_prevloans_merge = merge_prevloans(df_prevloans.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cdc91a9-4c60-4c9b-868c-50dd14ef346e",
      "metadata": {
        "id": "9cdc91a9-4c60-4c9b-868c-50dd14ef346e"
      },
      "outputs": [],
      "source": [
        "# drop unncessary columns\n",
        "df_prevloans_merge = df_prevloans_merge.drop(columns=['systemloanid', 'loannumber', 'approveddate', 'creationdate', 'loanamount', 'totaldue', 'termdays',\n",
        "                                                      'firstduedate', 'closeddate', 'firstrepaiddate', 'loan_duration', 'term_delay', 'no_default',\n",
        "                                                      'early', 'on_time', '0', '1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59c387da-45c0-4e98-917b-8a66a55cb73d",
      "metadata": {
        "id": "59c387da-45c0-4e98-917b-8a66a55cb73d"
      },
      "outputs": [],
      "source": [
        "df_prevloans_merge = df_prevloans_merge.drop_duplicates(subset='customerid', ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bacc626-6967-4f14-a29c-1ce3e69ec387",
      "metadata": {
        "id": "8bacc626-6967-4f14-a29c-1ce3e69ec387"
      },
      "outputs": [],
      "source": [
        "df_prevloans_merge.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d5fc215-d623-4113-b090-e3fd9d1a597c",
      "metadata": {
        "id": "1d5fc215-d623-4113-b090-e3fd9d1a597c"
      },
      "outputs": [],
      "source": [
        "df_prevloans_merge['customerid'].duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae51e7e5-b7e9-4247-8f6e-d46a82297616",
      "metadata": {
        "id": "ae51e7e5-b7e9-4247-8f6e-d46a82297616"
      },
      "source": [
        "The created dataframe df_prevloans_merge is now suitable for merging with the demographics and performance dataframes because all customer id's are unique."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f79c7e1-bacf-4963-b3d5-317762aba2ea",
      "metadata": {
        "id": "2f79c7e1-bacf-4963-b3d5-317762aba2ea"
      },
      "source": [
        "#### For Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bc3f518-2d10-4821-b66a-b4bc5791a9ad",
      "metadata": {
        "id": "4bc3f518-2d10-4821-b66a-b4bc5791a9ad"
      },
      "outputs": [],
      "source": [
        "# features specific to previous loans\n",
        "def features_prevloans(data):\n",
        "    # apply customer default function\n",
        "    data = loan_default(data)\n",
        "\n",
        "    # merge demographics\n",
        "    data = pd.merge(data, df_demographics, how='left', on='customerid')\n",
        "    data = data.dropna()\n",
        "\n",
        "    # repeat borrower\n",
        "    data['repeat_borrower'] = np.where(data['loannumber'] > 1, 1, 0)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59e5f5b-bcaa-46e7-a3b9-34e7bd27df7e",
      "metadata": {
        "id": "c59e5f5b-bcaa-46e7-a3b9-34e7bd27df7e"
      },
      "outputs": [],
      "source": [
        "# general features - that can also be applied to the merged data later on\n",
        "def features_general(data):\n",
        "    # date related\n",
        "    # approval speed\n",
        "    data['approval_speed_hours'] = (((data['approveddate'] - data['creationdate']).dt.seconds) / 3600).round()\n",
        "    # approval hour\n",
        "    data['approval_hour'] = data['approveddate'].dt.hour\n",
        "    # approval day\n",
        "    data['approval_day'] = data['approveddate'].dt.day_name()\n",
        "    # loan year\n",
        "    data['loan_year'] = data['creationdate'].dt.year\n",
        "    # loan month\n",
        "    data['loan_month'] = data['creationdate'].dt.month_name()\n",
        "\n",
        "    # loan related\n",
        "    # interest amount\n",
        "    data['interest_amount'] = data['totaldue'] - data['loanamount']\n",
        "    # interest rate\n",
        "    data['interest_rate'] = (data['interest_amount'] / data['totaldue']).round(4)\n",
        "    # loan amount category\n",
        "    data['loan_amount_category'] = pd.cut(data['loanamount'], bins=[0, 10000.0, 30000.0, 60000.0], labels=['Small', 'Medium', 'Large'])\n",
        "    # loan number category\n",
        "    data['loan_number_category'] = pd.cut(data['loannumber'], bins=[0, 5, 15, 30], labels=['Low', 'Mid', 'High'])\n",
        "\n",
        "    # client/demographics related\n",
        "    # age\n",
        "    data['age'] = data['creationdate'].dt.year - data['birthdate'].dt.year\n",
        "    # age group\n",
        "    data['age_group'] = pd.cut(data['age'], bins=[17, 25, 40, 55, 100], labels=['Young Adults', 'Adults', 'Middle Age', 'Seniors'])\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c74ab50c-8053-4c44-93e8-850408a06fec",
      "metadata": {
        "id": "c74ab50c-8053-4c44-93e8-850408a06fec"
      },
      "source": [
        "From the understanding of the previous loans dataframe, the following features were created for analysis of past loans:\n",
        "- no_default – Binary indicator showing whether the customer did not default on a loan: 1 = yes, 0 = no\n",
        "- loan_year – The year when the loan was approved.\n",
        "- loan_month – The month when the loan was approved.\n",
        "- approval_hour – Hour of the day (0–23) when the loan was approved.\n",
        "- approval_speed_hours – Time taken (in hours) between loan application creation and loan approval.\n",
        "- approval_day – Day of the week when the loan was approved.\n",
        "- interest_amount – Amount charged as interest for the loan.\n",
        "- interest_rate – Percentage rate charged as interest, based on the loan amount.\n",
        "- loan_amount_category – Categorical grouping of loan amounts (Small, Medium, Large).\n",
        "- loan_number_category – Categorical grouping based on how many loans the customer has taken so far (Low, Mid, High)\n",
        "- age – Customer’s age in years.\n",
        "- age_group – Categorical grouping of customers based on age ranges (Young adults 18-25, Adults 26-40, Middle age 41-55, Seniors 56+).\n",
        "- repeat_borrower – Binary indicator showing whether the customer has taken more than one loan: 1 = yes, 0 = no"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64e6819a-bcbc-42e6-a087-e8e9b604a379",
      "metadata": {
        "id": "64e6819a-bcbc-42e6-a087-e8e9b604a379"
      },
      "outputs": [],
      "source": [
        "# apply\n",
        "df_prevloans = features_prevloans(df_prevloans)\n",
        "df_prevloans = features_general(df_prevloans)\n",
        "\n",
        "# drop unnecessary columns\n",
        "df_prevloans = df_prevloans.drop(columns=['systemloanid', 'approveddate', 'creationdate', 'closeddate', 'firstduedate', 'firstrepaiddate',\n",
        "                                         'loan_duration', 'term_delay', 'birthdate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e29112-3037-41af-9385-cb1d2f23dae8",
      "metadata": {
        "id": "02e29112-3037-41af-9385-cb1d2f23dae8"
      },
      "source": [
        "### Merging the Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a33772f-4fd1-44b8-8403-023f04a59ca5",
      "metadata": {
        "id": "7a33772f-4fd1-44b8-8403-023f04a59ca5"
      },
      "outputs": [],
      "source": [
        "# merge df_perf and df_demographics\n",
        "df_temp = pd.merge(df_perf, df_demographics, how='left', on='customerid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd625992-ee1e-4269-b899-04a29f550fea",
      "metadata": {
        "id": "dd625992-ee1e-4269-b899-04a29f550fea"
      },
      "outputs": [],
      "source": [
        "# merge df_temp and df_prevloans\n",
        "df = pd.merge(df_temp, df_prevloans_merge, how='left', on='customerid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82e64ddf-4594-4f75-9254-f5b0732f7a47",
      "metadata": {
        "id": "82e64ddf-4594-4f75-9254-f5b0732f7a47"
      },
      "source": [
        "### Feature Engineering - Main Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ab5c8c-4722-4305-8feb-e14e4fc8ab63",
      "metadata": {
        "id": "29ab5c8c-4722-4305-8feb-e14e4fc8ab63"
      },
      "outputs": [],
      "source": [
        "# apply the general features function\n",
        "df = features_general(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ed904e0-04ab-4ba0-86b8-f77f7deb7931",
      "metadata": {
        "id": "8ed904e0-04ab-4ba0-86b8-f77f7deb7931"
      },
      "outputs": [],
      "source": [
        "# drop date columns\n",
        "df = df.drop(columns=['approveddate', 'creationdate', 'birthdate'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524d269f-f06a-4fb7-bff4-6fe255e90d2e",
      "metadata": {
        "id": "524d269f-f06a-4fb7-bff4-6fe255e90d2e"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "- Understand trends, patterns and  distributions from past loans\n",
        "- Understand trends, patterns and distributions of the main dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cf4a58b-9760-4635-b7c1-bc028f6c4028",
      "metadata": {
        "id": "0cf4a58b-9760-4635-b7c1-bc028f6c4028"
      },
      "source": [
        "### EDA - Previous Loans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b70a126-7bcc-4a30-b7ab-f49757cb5bcb",
      "metadata": {
        "id": "1b70a126-7bcc-4a30-b7ab-f49757cb5bcb"
      },
      "outputs": [],
      "source": [
        "#df_prevloans.to_csv('previous_loans.csv') - for power bi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a920dd77-177d-4dd8-98bf-86ef011fe489",
      "metadata": {
        "id": "a920dd77-177d-4dd8-98bf-86ef011fe489"
      },
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "df_prevloans.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca90a9b7-f1c2-46a9-b5ad-0a78721307bc",
      "metadata": {
        "id": "ca90a9b7-f1c2-46a9-b5ad-0a78721307bc"
      },
      "outputs": [],
      "source": [
        "# split into numerical, categorical and geographical columns\n",
        "num_cols = ['loannumber', 'loanamount', 'totaldue', 'approval_speed_hours', 'approval_hour', 'interest_amount', 'interest_rate', 'age']\n",
        "\n",
        "cat_cols = ['termdays', 'bank_account_type', 'bank_name_clients', 'employment_status_clients', 'loan_year', 'loan_month', 'approval_day',\n",
        "            'loan_amount_category', 'loan_number_category', 'age_group', 'repeat_borrower']\n",
        "\n",
        "geo_cols = ['longitude_gps', 'latitude_gps']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "117340bb-9bd8-4064-a621-f298e119d409",
      "metadata": {
        "id": "117340bb-9bd8-4064-a621-f298e119d409"
      },
      "source": [
        "#### Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ae33d1-d5dd-4254-9084-1e4883d556f6",
      "metadata": {
        "id": "46ae33d1-d5dd-4254-9084-1e4883d556f6"
      },
      "outputs": [],
      "source": [
        "# summary statistics\n",
        "df_prevloans[num_cols].describe().round(2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0805c1-fa9b-48ab-9efd-8dc7e4ac6916",
      "metadata": {
        "id": "cd0805c1-fa9b-48ab-9efd-8dc7e4ac6916"
      },
      "source": [
        "##### **Insights:**\n",
        "In the given dataset:\n",
        "- The minimum number of loans a customer has taken is 1, the maximum is 26\n",
        "- The minimum loan amount is 3000.0, the maximum is 60000.0\n",
        "- The minimum as well as the most common/average approval is 1 hour, the maximum is 24 hours (1 day)\n",
        "- The minimum approval hour is 00:00(12am), the maximum is 23:00(11pm).\n",
        "- The minimum interest amount added to a loan is 225.0, the maximum is 9000.0\n",
        "- The minimum interest rate is 0.02 percent, the maximum is 0.23 percent\n",
        "- The minimum age is 21 years, the maximum is 56 years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9691b1-a9fd-44f4-8d4d-a19a76904faa",
      "metadata": {
        "id": "7a9691b1-a9fd-44f4-8d4d-a19a76904faa"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr_matrix = df_prevloans[num_cols].corr()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt='.3f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9625f21-6794-4483-b79b-737b1f8da3c3",
      "metadata": {
        "id": "e9625f21-6794-4483-b79b-737b1f8da3c3"
      },
      "source": [
        "##### **Top 3 Strongest Positive Correlations (as one increases, the other increases):**\n",
        "1. totaldue and loanamount = 0.995: Larger loan amounts result in higher total due\n",
        "2. loanamount and loannumber = 0.795: Customers taking more loans also tend to borrow larger amounts.\n",
        "3. totaldue and interest_amount = 0.788: As the total due increases, the interest amount also increases.\n",
        "\n",
        "##### **Top 3 Strongest Negative Correlations (as one increases, the other decreases):**\n",
        "1. loannumber and interest_rate = -0.446: The more loans a customer takes, the lower the interest rate tends to be, suggesting loyal or repeat customers receive better rates.\n",
        "2. loanamount and interest_rate = -0.430: Higher loan amounts are mostly associated with lower interest rates\n",
        "3. totaldue and interest_rate = -0.352: Customers who owe more overall (total due) generally face lower interest rates\n",
        "\n",
        "The correlation matrix helps understand how the different numerical features interact with/affect each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67d279c-933e-48df-b7df-82d164de0ebf",
      "metadata": {
        "id": "b67d279c-933e-48df-b7df-82d164de0ebf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# num_cols distribution and boxplot - loannumber, loanamount, age and approval speed\n",
        "num_cols = ['loannumber', 'loanamount', 'approval_speed_hours', 'age']\n",
        "for col in num_cols:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    # histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df_prevloans[col], bins=30, kde=True)\n",
        "    plt.title(f\"{col.capitalize()} Distribution\")\n",
        "    plt.xlabel(col.capitalize())\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    #boxplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x=df_prevloans[col], color='green')\n",
        "    plt.title(f\"{col.capitalize()} Boxplot\")\n",
        "    plt.xlabel(col.capitalize())\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab242e9-f95b-4053-b498-dd046a0144e3",
      "metadata": {
        "id": "0ab242e9-f95b-4053-b498-dd046a0144e3"
      },
      "source": [
        "##### **Insights:**\n",
        "- The majority of customers took only one loan.\n",
        "- A loan amount of 10000.0 was the most common among customers.\n",
        "- On average, the banks approved loans within one hour of creation.\n",
        "- Most customers were primarily in their 30s."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdfc602-f17b-43b7-b712-cbdff26566a1",
      "metadata": {
        "id": "cfdfc602-f17b-43b7-b712-cbdff26566a1"
      },
      "source": [
        "#### Categorical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d749a218-0482-48fc-b31a-5e854cbe1187",
      "metadata": {
        "id": "d749a218-0482-48fc-b31a-5e854cbe1187",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# cat cols visualization\n",
        "fig, axes = plt.subplots(6, 2, figsize=(12, 25))\n",
        "\n",
        "for i, col in enumerate(cat_cols):\n",
        "    ax = axes[i//2, i%2]\n",
        "    sns.countplot(data=df_prevloans, x=col, ax=ax)\n",
        "    ax.set_title(f\"{col.capitalize()} Countplot\")\n",
        "    ax.set_xlabel(col.capitalize())\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.tick_params(axis='x', rotation=90)\n",
        "    axes[5,1].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adcb4829-7a63-46d1-bb9c-ce237ecc7a78",
      "metadata": {
        "id": "adcb4829-7a63-46d1-bb9c-ce237ecc7a78"
      },
      "source": [
        "##### **Insights:**\n",
        "- The most common term of a loan is 30 days. The least common term of a loan is 90  days\n",
        "- The most common account type is a Savings account. The least common account type is a Current account\n",
        "- GT Bank has the highest number of loan customers. Unity Bank has the lowest number of loan customers\n",
        "- The employment status with the most customers is 'Permanent'. The employment status with the least customers is 'Contract'\n",
        "- The month with the most loan activity is June. The month with the least loan activity is September\n",
        "- The day with the most loan activity is Wednesday. The day with the least loan activity is Sunday\n",
        "- Customers took more than one loan, with most having a small number of loans taken (0 - 5)\n",
        "- Customers took smaller loan amounts (0 - 10000.0).\n",
        "- Customers were mostly Adults (26 - 40 years)\n",
        "- There are only 2 years in the dataset, 2016 and 2017. More loans were taken in 2017 than 2016\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7487cba3-5c1f-424a-8455-9421212aabec",
      "metadata": {
        "id": "7487cba3-5c1f-424a-8455-9421212aabec"
      },
      "source": [
        "#### Interest Amount/Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61a21a9-7faa-4188-9249-5ef25b72564a",
      "metadata": {
        "id": "a61a21a9-7faa-4188-9249-5ef25b72564a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# check for differences in the interest amount\n",
        "pd.set_option('display.max_rows', None)\n",
        "df_prevloans.groupby(['loanamount', 'termdays', 'bank_name_clients'], observed=True )['interest_amount'].agg(['min', 'max'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different interest amounts were observed for the same amount + same term + same bank"
      ],
      "metadata": {
        "id": "GrKvETd774xW"
      },
      "id": "GrKvETd774xW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b72068e-6c71-4043-9a0d-3eb859ba2961",
      "metadata": {
        "id": "2b72068e-6c71-4043-9a0d-3eb859ba2961",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# check reason for different interest amounts/rates per bank\n",
        "\n",
        "df_prevloans.groupby(['bank_name_clients', 'loanamount', 'termdays', 'age_group', 'employment_status_clients', 'bank_account_type', 'no_default',\n",
        "                      'loannumber', 'loan_year', 'loan_month'], observed=True)['interest_amount'].agg(['min', 'max'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prevloans.loc[df_prevloans['loannumber'] == 1]"
      ],
      "metadata": {
        "id": "aG3_CYsPLBME"
      },
      "id": "aG3_CYsPLBME",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prevloans.loc[df_prevloans['loannumber'] == 1, 'loanamount'].max()"
      ],
      "metadata": {
        "id": "kBoI7_xnJcl3"
      },
      "id": "kBoI7_xnJcl3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.reset_option('display.max_rows')"
      ],
      "metadata": {
        "id": "OSzPDFx_LQtc"
      },
      "id": "OSzPDFx_LQtc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "69863f54-3b2a-4ef4-a0aa-c9c639d6dc1a",
      "metadata": {
        "id": "69863f54-3b2a-4ef4-a0aa-c9c639d6dc1a"
      },
      "source": [
        "##### **Key Finding:**\n",
        "The variation in interest rates among borrowers with the same loan amount and term can be explained by the interaction between loan number (loannumber) and repayment behavior (no_default)\n",
        "\n",
        "It was observed that for:\n",
        "1. Consistent Repayers (no_default = 1 or Low/0 default rate)\n",
        "    - Interest rates decrease as loan number increases. This suggests banks use loan history as a trust and loyalty mechanism.\n",
        "    - Borrowers who demonstrate reliability are rewarded with lower rates over time.\n",
        "2. Defaulting Borrowers (no_default = 0 or High default rate)\n",
        "    - Interest rates remain the same across loan numbers.\n",
        "    - High number of past defaults prevent access to loyalty benefits.\n",
        "\n",
        "Notably, the reduction in interest rate typically begins around the 4th loan, though this may vary depending on the customer’s repayment history (default rate).\n",
        "\n",
        "This finding confirms the observed correlation between loannumber and interest_rate = -0.446\n",
        "\n",
        "##### **Further Insights:**\n",
        "- Interest rate increases as term days increases\n",
        "- The higher the number of loans a customer has taken, the higher the loan amount the customer can borrow.\n",
        "- The highest loan amount a customer can take without having a history with the bank is 10000.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d85b9329-1bb1-490f-835e-2452fed6de08",
      "metadata": {
        "id": "d85b9329-1bb1-490f-835e-2452fed6de08"
      },
      "source": [
        "#### Rate of Defaults Based On Different Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7dff5ac-4596-4dd8-be50-3daa0a4da19c",
      "metadata": {
        "id": "f7dff5ac-4596-4dd8-be50-3daa0a4da19c"
      },
      "outputs": [],
      "source": [
        "# features to use for comparison\n",
        "features = ['loannumber', 'age', 'age_group', 'termdays', 'loanamount', 'bank_account_type', 'bank_name_clients',\n",
        "            'employment_status_clients', 'loan_month', 'loan_year', 'approval_day', 'repeat_borrower', 'approval_speed_hours', 'interest_amount',\n",
        "            'loan_amount_category', 'loan_number_category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba97f6a-d135-4065-85b5-32e6b154fa9b",
      "metadata": {
        "id": "9ba97f6a-d135-4065-85b5-32e6b154fa9b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# visualise\n",
        "fig, axes = plt.subplots(8, 2, figsize=(15, 28), constrained_layout=True)\n",
        "\n",
        "for i, feat in enumerate(features):\n",
        "    # calculate\n",
        "    percent_target = df_prevloans.groupby([feat, 'no_default'], observed=True)['no_default'].count() / df_prevloans.groupby(feat, observed=True)['no_default'].count() * 100\n",
        "    # change to a data frame\n",
        "    percent_target = percent_target.reset_index(name='percentage')\n",
        "    # get dataframe of bad flags only\n",
        "    default_percent = percent_target[percent_target['no_default'] == 0]\n",
        "\n",
        "    #plot\n",
        "    ax = axes[i//2, i%2]\n",
        "    sns.barplot(data=default_percent, x=feat, y='percentage', ax=ax)\n",
        "    ax.set_title(f\"Default Rate per {feat.capitalize()}\")\n",
        "    ax.set_xlabel(feat.capitalize())\n",
        "    ax.set_ylabel('Default Rate')\n",
        "    ax.tick_params(axis='x', rotation=90)\n",
        "\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792f551d-1305-4363-aa8a-4c02bf92043f",
      "metadata": {
        "id": "792f551d-1305-4363-aa8a-4c02bf92043f"
      },
      "source": [
        "##### **Insights:**\n",
        "- People defaulted more on their 2nd loans, then their 11th. Lowest: 20 - 26 with 0%. Having a high loan number (20 - 26) is only possible with a good repayment history.\n",
        "- Clients aged 22 years had the highest default rate, followed by 43 years. Lowest: 56 years with 0% (one person). Next Lowest:55 years\n",
        "- Clients in the Young Adults age group (18 - 25 years) had the highest default rate. Lowest: Seniors (56 years and above). Next Lowest: Adults (26 - 40 years)\n",
        "- Loans with a 30 day term had the highest default rates, followed by 15 days. Lowest: 90 days with 0%. A longer term alludes to a higher loan amount, higher loan number and a good repayment history.\n",
        "- Loan amounts of 9000 had the highest default rate, then 4000. Lowest: 60,000. A customer can only take a higher loans if they have a good repayment history of the smaller loans\n",
        "- Clients with a Savings account had the highest default rate, followed by Other. Lowest: Current\n",
        "- Sterling Bank had the highest default rate, then Eco Bank. Lowest: Unity Bank\n",
        "- Unemployed clients had the highest default rate, then Self-employed. Lowest: Contract workers (one person), Next Lowest: Retired workers.\n",
        "- The month of September had the highest default rate, then May. Lowest: July. September is a known month for school resumption, this could have affected some customers ability to pay back.\n",
        "- The year 2016 had a higher default rate. Upon researching, I found out that the year 2016 had significant financial problems, a major one being a global economic slowdown, with world GDP growth dropping to 2.3%, down from 2.7% the previous year. This could have affected customers abilities to pay back\n",
        "- Approval speeds of 20 hours and 24 hours had the highest default rate. This suggests that higher approval speeds give a higher chance of default.\n",
        "- Smaller loan amounts had the highest default rates. Considering banks only allow higher loans for repeat customers with good repayment history, this is to be expected. Smaller loan amounts are like the 'starting points' for loans.\n",
        "- Customers with a low loan number (0-5) had the highest default rates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed7601d7-75a4-4e12-ac7f-e331b9f438c7",
      "metadata": {
        "id": "ed7601d7-75a4-4e12-ac7f-e331b9f438c7"
      },
      "source": [
        "#### Geographical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3a53df-fe71-4a0d-bc5d-acc33f1eb376",
      "metadata": {
        "id": "db3a53df-fe71-4a0d-bc5d-acc33f1eb376"
      },
      "outputs": [],
      "source": [
        "# get data from prevloans to use for geographic analysis\n",
        "df_geo = df_prevloans.loc[:, geo_cols + ['no_default']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0018160-3427-4fa4-9c13-45cb9731b746",
      "metadata": {
        "id": "e0018160-3427-4fa4-9c13-45cb9731b746"
      },
      "outputs": [],
      "source": [
        "# create a geo dataframe\n",
        "df_geo = gpd.GeoDataFrame(df_geo, geometry=gpd.points_from_xy(df_geo.longitude_gps, df_geo.latitude_gps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de36e67-cf6f-47c1-8763-6943f1c822f3",
      "metadata": {
        "id": "7de36e67-cf6f-47c1-8763-6943f1c822f3"
      },
      "outputs": [],
      "source": [
        "df_geo.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35f3125-7172-4dfc-a2a3-b2ae68a1e4ef",
      "metadata": {
        "id": "d35f3125-7172-4dfc-a2a3-b2ae68a1e4ef"
      },
      "outputs": [],
      "source": [
        "df_geo.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "462a279e-df1b-4655-a757-531a8250f84d",
      "metadata": {
        "id": "462a279e-df1b-4655-a757-531a8250f84d"
      },
      "outputs": [],
      "source": [
        "# get world country data geojson\n",
        "url = 'https://raw.githubusercontent.com/martynafford/natural-earth-geojson/refs/heads/master/110m/cultural/ne_110m_admin_0_countries.json'\n",
        "world = gpd.read_file(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bbc6bae-3e4d-4195-b055-2a16600936fc",
      "metadata": {
        "id": "8bbc6bae-3e4d-4195-b055-2a16600936fc"
      },
      "outputs": [],
      "source": [
        "# rename columns to match 'naturalearth_lowres'\n",
        "rename_mapper = {\n",
        "    'POP_EST': 'pop_est',\n",
        "    'CONTINENT': 'continent',\n",
        "    'NAME': 'name',\n",
        "    'ADM0_A3': 'iso_a3',\n",
        "    'GDP_MD_EST': 'gdp_md_est'\n",
        "}\n",
        "world_data = world.rename(columns=rename_mapper)\n",
        "\n",
        "# keep only the columns that naturalearth_lowres had\n",
        "world_data = world_data.loc[:, ['pop_est', 'continent', 'name', 'iso_a3', 'gdp_md_est', 'geometry']]\n",
        "\n",
        "world_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8c0905-b0b3-408b-9ccd-948501c04c8e",
      "metadata": {
        "id": "4a8c0905-b0b3-408b-9ccd-948501c04c8e"
      },
      "outputs": [],
      "source": [
        "# set crs of the dataframe to match world_data\n",
        "df_geo = df_geo.set_crs(epsg=4326)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e34aae2-84b4-4644-99aa-dbc288205ffd",
      "metadata": {
        "id": "3e34aae2-84b4-4644-99aa-dbc288205ffd"
      },
      "outputs": [],
      "source": [
        "# check continents contained in the loan data\n",
        "df_geo_continents = gpd.sjoin(df_geo, world_data[['continent', 'geometry', 'name', 'iso_a3']], how='inner')\n",
        "df_geo_continents['continent'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1021fb-3951-494e-bc77-15a2b0d0b169",
      "metadata": {
        "id": "2f1021fb-3951-494e-bc77-15a2b0d0b169"
      },
      "outputs": [],
      "source": [
        "# check countries contained in the loan data and their default counts\n",
        "geo_countries = pd.crosstab([df_geo_continents['continent'], df_geo_continents['name']], df_geo_continents['no_default'])\n",
        "geo_countries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e7a97c-1d02-4ba6-9d1b-d67325a136c1",
      "metadata": {
        "id": "a3e7a97c-1d02-4ba6-9d1b-d67325a136c1"
      },
      "source": [
        "Considering there are more good flags than bad flags (no default vs default) in the data as a whole, i'll be focusing on the defaults alone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6200c39-35b0-4450-9acd-7b0f26407d40",
      "metadata": {
        "id": "c6200c39-35b0-4450-9acd-7b0f26407d40"
      },
      "outputs": [],
      "source": [
        "# create a function to calculate default rates on a crosstab df\n",
        "def crosstab_data(df_crosstab):\n",
        "    # reset index\n",
        "    df = df_crosstab.reset_index()\n",
        "\n",
        "    # calculate total loan count and default rate\n",
        "    df['total_count'] = df[0] + df[1]  # 1 = no default, 0 = default\n",
        "    df['default_count'] = df[0]\n",
        "    df['no_default_count'] = df[1]\n",
        "    df['default_rate'] = df[0] / df['total_count']\n",
        "    df['default_rate_pct'] = df['default_rate'] * 100\n",
        "\n",
        "    # clean column names\n",
        "    df = df.drop(columns=[0, 1])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a85d33e-c535-48cf-867f-4e43917cfcda",
      "metadata": {
        "id": "1a85d33e-c535-48cf-867f-4e43917cfcda"
      },
      "outputs": [],
      "source": [
        "geo_countries = crosstab_data(geo_countries)\n",
        "geo_countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "075dde8d-7d26-435f-82c8-38c33d60e1d4",
      "metadata": {
        "id": "075dde8d-7d26-435f-82c8-38c33d60e1d4"
      },
      "outputs": [],
      "source": [
        "# get continent data to plot choropleth map on default rates (percentage)\n",
        "continent_choropleth = world_data.merge(geo_countries, on = ['continent', 'name'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f584a5ee-ce20-433f-976d-3155f84cc45c",
      "metadata": {
        "id": "f584a5ee-ce20-433f-976d-3155f84cc45c"
      },
      "outputs": [],
      "source": [
        "# change to geojson\n",
        "continents_geojson = continent_choropleth.__geo_interface__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c5b1a0-a62b-49a4-8665-27e0af256ec1",
      "metadata": {
        "id": "f6c5b1a0-a62b-49a4-8665-27e0af256ec1"
      },
      "outputs": [],
      "source": [
        "# plot total loans taken per country\n",
        "fig = px.choropleth(\n",
        "    continent_choropleth,\n",
        "    geojson=continents_geojson,\n",
        "    locations=continent_choropleth.index,\n",
        "    color= 'total_count',\n",
        "    hover_name= 'name',\n",
        "    hover_data={\n",
        "        'continent': True,\n",
        "        'total_count': True,\n",
        "    },\n",
        "    color_continuous_scale='Teal',\n",
        "    title='Geographical Distribution of Loans Taken'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hover over the map to view country name, continent and loan count"
      ],
      "metadata": {
        "id": "wVlUl1QGCvHX"
      },
      "id": "wVlUl1QGCvHX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e1aa44-ea8d-4165-a82b-3f180e77cca8",
      "metadata": {
        "id": "d1e1aa44-ea8d-4165-a82b-3f180e77cca8"
      },
      "outputs": [],
      "source": [
        "# plot default rates (percentage) per country\n",
        "fig = px.choropleth(\n",
        "    continent_choropleth,\n",
        "    geojson=continents_geojson,\n",
        "    locations=continent_choropleth.index,\n",
        "    color= 'default_rate_pct',\n",
        "    hover_name= 'name',\n",
        "    hover_data={\n",
        "        'continent': True,\n",
        "        'total_count': True,\n",
        "        'default_count': True,\n",
        "        'default_rate_pct': ':.2f'\n",
        "    },\n",
        "    color_continuous_scale='Teal',\n",
        "    title='Geographical Distribution of Default Rate'\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hover over the map to view country name, continent, loan count, default count and default rate (percentage)"
      ],
      "metadata": {
        "id": "FyoV42AADN1m"
      },
      "id": "FyoV42AADN1m"
    },
    {
      "cell_type": "markdown",
      "id": "ab957214-535b-4e47-8561-6ccbb53aea71",
      "metadata": {
        "id": "ab957214-535b-4e47-8561-6ccbb53aea71"
      },
      "source": [
        "##### **Statistics/Insights:**\n",
        "There are 6 coordinates and 37 loans without a continent/country match in the world data.\n",
        "\n",
        "From the 12,273 loans that do have a match, I have inferred that customers are located in 5 continents at different concentrations in different countries with the most number of customers being from Nigeria, Africa.\n",
        "\n",
        "The continents and the total number of loans taken and the breakdown per country are as follows: (all default rates are rounded to the nearest whole number excluding 0 defaults, 100% defaults or unless stated otherwise)\n",
        "\n",
        "Africa with 12,176 total loans divided into:\n",
        "  - Nigeria - 12,137 total loans, 2506 defaults, 21% default rate\n",
        "  - Ghana - 18 total loans, 4 defaults, 22% default rate\n",
        "  - Benin - 10 total loans, 1 default, 10% default rate (exact)\n",
        "  - Côte d'Ivoire - 5 total loans, 1 default, 20% default rate (exact)\n",
        "  - Niger - 4 total loans, 3 defaults, 75% default rate (exact)\n",
        "  - Cameroon - 2 total loans, 0 defaults, 0% default rate\n",
        "    \n",
        "North America with 55 loans divided into:\n",
        "  - United States of America - 55 loans, 13 defaults, 24% default rate\n",
        "    \n",
        "Europe with 25 loans divided into:\n",
        "  - United Kingdom - 15 loans, 8 defaults, 53% default rate\n",
        "  - Russia - 10 loans, 0 defaults, 0% default rate\n",
        "    \n",
        "Asia with 10 loans divided into:\n",
        "  - United Arab Emirates - 5 loans, 2 defaults, 40% default rate (exact)\n",
        "  - Thailand - 4 loans, 0 defaults, 0% default rate\n",
        "  - China - 1 loan, 1 default, 100% default rate\n",
        "    \n",
        "Oceania with 7 loans divided into:\n",
        "  - Australia - 7 loans, 1 default, 14% default rate\n",
        "\n",
        "Apart from Nigeria, the number of loans taken per country is too low for a reliable inference on default rates to be gotten or to get deeper insights like default by region or default by state. However the above information can be used to make decisions such as no longer accepting loans from Niger based on the business goals or preference.\n",
        "\n",
        "I will now provide further insights on default rates in Nigeria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05de1582-41ed-43c3-ad11-bce62c3ab96e",
      "metadata": {
        "id": "05de1582-41ed-43c3-ad11-bce62c3ab96e"
      },
      "outputs": [],
      "source": [
        "# get world regional data - admin_1\n",
        "url2 = 'https://raw.githubusercontent.com/martynafford/natural-earth-geojson/refs/heads/master/10m/cultural/ne_10m_admin_1_states_provinces.json'\n",
        "admin_1 = gpd.read_file(url2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ac3ac5-ea62-479d-8426-f5cc86feb64f",
      "metadata": {
        "id": "c3ac3ac5-ea62-479d-8426-f5cc86feb64f"
      },
      "outputs": [],
      "source": [
        "# filter for Nigeria only\n",
        "nigeria_admin1 = admin_1[admin_1['admin'] == 'Nigeria']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b82e5c64-873c-413d-80dd-0bfee35345fa",
      "metadata": {
        "id": "b82e5c64-873c-413d-80dd-0bfee35345fa"
      },
      "outputs": [],
      "source": [
        "# create geodataframe\n",
        "nigeria_gdf = (gpd.sjoin(df_geo, nigeria_admin1[['name', 'admin', 'adm1_code', 'postal', 'geometry']], how='left'))\n",
        "nigeria_gdf = nigeria_gdf.set_geometry(df_geo.geometry)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1034c0c0-4619-434d-82a8-4fcaa3654869",
      "metadata": {
        "id": "1034c0c0-4619-434d-82a8-4fcaa3654869"
      },
      "outputs": [],
      "source": [
        "# rows that didn’t match any state in the data\n",
        "missing = nigeria_gdf[nigeria_gdf['name'].isna()].drop_duplicates()\n",
        "\n",
        "print(f\"There are {len(missing)} locations without a state match, including the initial 37\")\n",
        "print(missing[['longitude_gps','latitude_gps']].head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f7ce6f-c6d0-48c7-9647-752d88c48460",
      "metadata": {
        "id": "a9f7ce6f-c6d0-48c7-9647-752d88c48460"
      },
      "outputs": [],
      "source": [
        "# add region column\n",
        "\n",
        "# create a dictionary with all regions\n",
        "region_map = {\n",
        "    \"North West\": [\"Kano\", \"Kaduna\", \"Katsina\", \"Kebbi\", \"Jigawa\", \"Sokoto\", \"Zamfara\"],\n",
        "    \"North East\": [\"Borno\", \"Adamawa\", \"Yobe\", \"Taraba\", \"Bauchi\", \"Gombe\"],\n",
        "    \"North Central\": [\"Kogi\", \"Nasarawa\", \"Benue\", \"Niger\", \"Kwara\", \"Plateau\", \"FCT Abuja\"],\n",
        "    \"South West\": [\"Lagos\", \"Oyo\", \"Ogun\", \"Ondo\", \"Osun\", \"Ekiti\"],\n",
        "    \"South East\": [\"Anambra\", \"Imo\", \"Abia\", \"Ebonyi\", \"Enugu\"],\n",
        "    \"South South\": [\"Edo\", \"Delta\", \"Bayelsa\", \"Rivers\", \"Akwa Ibom\", \"Cross River\"],\n",
        "}\n",
        "\n",
        "# create a function that maps the region to a state\n",
        "def get_region(state):\n",
        "    for region, states in region_map.items():\n",
        "        if state in states:\n",
        "            return region\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "557c1e32-619a-4fc8-97e0-fdb26d8dbac4",
      "metadata": {
        "id": "557c1e32-619a-4fc8-97e0-fdb26d8dbac4"
      },
      "outputs": [],
      "source": [
        "# add region to nigeria gdf\n",
        "nigeria_gdf['region'] = nigeria_gdf['name'].apply(get_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9835ddc8-2989-4941-a539-6248e978a973",
      "metadata": {
        "id": "9835ddc8-2989-4941-a539-6248e978a973"
      },
      "outputs": [],
      "source": [
        "# set crs\n",
        "nigeria_gdf = nigeria_gdf.set_crs(epsg=4326)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da818b5f-57eb-4e23-8908-cff65e5371c6",
      "metadata": {
        "id": "da818b5f-57eb-4e23-8908-cff65e5371c6"
      },
      "outputs": [],
      "source": [
        "nigeria_gdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19d0d524-caf0-4ffc-85d8-a411c688ce1d",
      "metadata": {
        "id": "19d0d524-caf0-4ffc-85d8-a411c688ce1d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# default rate per state\n",
        "nigeria_states = pd.crosstab([nigeria_gdf['name'], nigeria_gdf['region']], nigeria_gdf['no_default'])\n",
        "nigeria_states = crosstab_data(nigeria_states)\n",
        "nigeria_states.columns = ['name', 'region', 'total_count', 'default_count', 'no_default_count', 'default_rate', 'default_rate_pct']\n",
        "nigeria_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2894b605-693a-4163-8d66-7070f2bc0779",
      "metadata": {
        "id": "2894b605-693a-4163-8d66-7070f2bc0779"
      },
      "outputs": [],
      "source": [
        "# default rate per region\n",
        "nigeria_regions = pd.crosstab(nigeria_gdf['region'], nigeria_gdf['no_default'])\n",
        "nigeria_regions = crosstab_data(nigeria_regions)\n",
        "nigeria_regions.columns = ['region', 'total_count', 'default_count', 'no_default_count', 'default_rate', 'default_rate_pct']\n",
        "nigeria_regions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad5f4454-85ad-48ef-8f84-447ade893247",
      "metadata": {
        "id": "ad5f4454-85ad-48ef-8f84-447ade893247"
      },
      "outputs": [],
      "source": [
        "# get state data to plot choropleth map on default rates (percentage)\n",
        "states_choropleth = nigeria_admin1.merge(nigeria_states, on = 'name', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b21b80b-2d9d-4e7c-baef-e6dd83bd04fd",
      "metadata": {
        "id": "1b21b80b-2d9d-4e7c-baef-e6dd83bd04fd"
      },
      "outputs": [],
      "source": [
        "# change to geojson\n",
        "nigeria_states_geojson = states_choropleth.__geo_interface__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot total loans per state in Nigeria\n",
        "fig = px.choropleth(\n",
        "    states_choropleth,\n",
        "    geojson=nigeria_states_geojson,\n",
        "    locations=states_choropleth.index,\n",
        "    color= 'total_count',\n",
        "    hover_name= 'name',\n",
        "    hover_data={\n",
        "        'total_count': True,\n",
        "    },\n",
        "    title='Loan Distribution by States in Nigeria',\n",
        "    color_continuous_scale='Greens',\n",
        ")\n",
        "fig.update_geos(fitbounds='locations', visible=False)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "urPIlDH9EEKs"
      },
      "id": "urPIlDH9EEKs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hover over the map to view state name and loan count"
      ],
      "metadata": {
        "id": "90SFVQLdE8-O"
      },
      "id": "90SFVQLdE8-O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884403e5-9920-48c1-b617-58785689b0e5",
      "metadata": {
        "id": "884403e5-9920-48c1-b617-58785689b0e5"
      },
      "outputs": [],
      "source": [
        "# plot default rates (percentage) per state in Nigeria\n",
        "fig = px.choropleth(\n",
        "    states_choropleth,\n",
        "    geojson=nigeria_states_geojson,\n",
        "    locations=states_choropleth.index,\n",
        "    color= 'default_rate_pct',\n",
        "    hover_name= 'name',\n",
        "    hover_data={\n",
        "        'total_count': True,\n",
        "        'default_count': True,\n",
        "        'default_rate_pct': ':.2f'\n",
        "    },\n",
        "    title='Loan Default Rate by States in Nigeria',\n",
        "    color_continuous_scale='Greens',\n",
        ")\n",
        "fig.update_geos(fitbounds='locations', visible=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04add603-31d7-4a4e-9e05-5639cb3ebba6",
      "metadata": {
        "id": "04add603-31d7-4a4e-9e05-5639cb3ebba6"
      },
      "source": [
        "Hover over the map to view state name, loan count, default count and default rate (percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c44e60-1fd9-48dd-8f0d-34ddf6caa50b",
      "metadata": {
        "id": "97c44e60-1fd9-48dd-8f0d-34ddf6caa50b"
      },
      "outputs": [],
      "source": [
        "# get regional data to plot choropleth map on default rates (percentage)\n",
        "nigeria_regions_gdf = nigeria_gdf.dissolve(by='region', as_index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ad98a6-d8e1-49bc-86f5-3c6607383e73",
      "metadata": {
        "id": "f0ad98a6-d8e1-49bc-86f5-3c6607383e73"
      },
      "outputs": [],
      "source": [
        "# map region to state\n",
        "nigeria_admin1.loc[:, 'region'] = nigeria_admin1['name'].apply(get_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9a9b70-6e21-48bc-8a87-aabea07fe74a",
      "metadata": {
        "id": "dc9a9b70-6e21-48bc-8a87-aabea07fe74a"
      },
      "outputs": [],
      "source": [
        "# aggregate shapes\n",
        "regions_admin1 = nigeria_admin1.dissolve(by='region', aggfunc='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "660f709b-145d-4ebf-aa03-89c013fc014d",
      "metadata": {
        "id": "660f709b-145d-4ebf-aa03-89c013fc014d"
      },
      "outputs": [],
      "source": [
        "# merge statistical data such as total_count\n",
        "regions_choropleth = regions_admin1.merge(nigeria_regions, how='left', on = 'region')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8440006-b12e-440d-a090-49eebeb36267",
      "metadata": {
        "id": "f8440006-b12e-440d-a090-49eebeb36267"
      },
      "outputs": [],
      "source": [
        "# convert to geojson\n",
        "nigeria_regions_geojson = regions_choropleth.__geo_interface__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot total loans per region in Nigeria\n",
        "fig = px.choropleth(\n",
        "    regions_choropleth,\n",
        "    geojson=nigeria_regions_geojson,\n",
        "    locations=regions_choropleth.index,\n",
        "    color= 'total_count',\n",
        "    hover_name= 'region',\n",
        "    hover_data={\n",
        "        'total_count': True,\n",
        "    },\n",
        "    title='Loan Distribution by Regions in Nigeria',\n",
        "    color_continuous_scale='Greens',\n",
        ")\n",
        "fig.update_geos(fitbounds='locations', visible=False)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oIPvlJvpEcie"
      },
      "id": "oIPvlJvpEcie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hover over the map to view region name and loan count"
      ],
      "metadata": {
        "id": "TuJGRJ24FFLY"
      },
      "id": "TuJGRJ24FFLY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08fd4767-1a67-4cc8-a43d-321665649371",
      "metadata": {
        "id": "08fd4767-1a67-4cc8-a43d-321665649371"
      },
      "outputs": [],
      "source": [
        "# plot default rates (percentage) per region in Nigeria\n",
        "fig = px.choropleth(\n",
        "    regions_choropleth,\n",
        "    geojson=nigeria_regions_geojson,\n",
        "    locations=regions_choropleth.index,\n",
        "    color= 'default_rate_pct',\n",
        "    hover_name= 'region',\n",
        "    hover_data={\n",
        "        'total_count': True,\n",
        "        'default_count': True,\n",
        "        'default_rate_pct': ':.2f'\n",
        "    },\n",
        "    title='Loan Default Rate by Regions in Nigeria',\n",
        "    color_continuous_scale='Greens',\n",
        ")\n",
        "fig.update_geos(fitbounds='locations', visible=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "659a717f-a453-48da-9f18-f330ede189e2",
      "metadata": {
        "id": "659a717f-a453-48da-9f18-f330ede189e2"
      },
      "source": [
        "Hover over the map to view region name, loan count, default count and default rate (percentage)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de48b777-3147-4257-b47e-2d69626b8115",
      "metadata": {
        "id": "de48b777-3147-4257-b47e-2d69626b8115"
      },
      "source": [
        "##### **Statistics:**\n",
        "Nigeria has the highest number of customers taking loans in the dataset. There are 145 locations without a state match, including the initial 37. From the matched coordinates in the data:\n",
        "- The region with the most customers is the South West Region - 7747 loans, the lowest being North East - 95 loans.\n",
        "- The state with the most customers is Lagos State - 5209 loans, the lowest being Zamfara and Jigawa - 3 loans.\n",
        "\n",
        "The regions, total number of loans taken and the breakdown per state is as follows: (all default rates are rounded to the nearest whole number excluding 0 defaults, 100% defaults or unless stated otherwise)\n",
        "\n",
        "**South West - 7747 loans, 1610 defaults, 21% default rate**\n",
        "- Lagos - 5209 loans, 1050 defaults, 20% default rate\n",
        "- Oyo - 1039 loans, 246 defaults, 24% default rate\n",
        "- Osun - 226 loans, 54 defaults, 24% default rate\n",
        "- Ondo - 279 loans, 41 defaults, 15% default rate\n",
        "- Ogun - 898 loans, 192 defaults, 21% default rate\n",
        "- Ekiti - 96 loans, 27 defaults, 28% default rate\n",
        "\n",
        "**South South - 1330 loans, 308 defaults, 23% default rate**\n",
        "- Rivers - 477 loans, 109 defaults, 23% default rate\n",
        "- Edo - 155 loans, 42 defaults, 27% default rate\n",
        "- Delta - 309 loans, 54 defaults, 17% default rate\n",
        "- Cross River - 108 loans, 33 defaults, 31% default rate\n",
        "- Bayelsa - 190 loans, 50 defaults, 26% default rate\n",
        "- Akwa Ibom - 91 loans, 20 defaults, 22% default rate\n",
        "\n",
        "**North Central - 684 loans, 123 defaults, 18% default rate**\n",
        "- Plateau - 109 loans, 17 defaults, 16% default rate\n",
        "- Niger - 71 loans, 8 defaults, 11% default rate\n",
        "- Kwara - 369 loans, 70 defaults, 19% default rate\n",
        "- Kogi - 47 loans, 11 defaults, 23% default rate\n",
        "- Benue - 88 loans, 17 defaults, 19% default rate\n",
        "\n",
        "**North West - 431 loans, 71 defaults, 16% default rate**\n",
        "- Zamfara - 3 loans, 0 defaults, 0% default rate\n",
        "- Sokoto - 19 loans, 2 defaults, 11% default rate\n",
        "- Jigawa - 3 loans, 0 defaults, 0% default rate\n",
        "- Kebbi - 4 loans, 0 defaults, 0% default rate\n",
        "- Katsina - 17 loans, 5 defaults, 29% default rate\n",
        "- Kano - 103 loans, 14 defaults, 14% default rate\n",
        "- Kaduna - 282 loans, 50 defaults, 18% default rate\n",
        "\n",
        "**South East - 382 loans, 83 defaults, 22% default rate**\n",
        "- Imo - 82 loans, 15 defaults, 18% default rate\n",
        "- Enugu - 124 loans, 20 defaults, 16% default rate\n",
        "- Ebonyi - 19 loans, 6 defaults, 32% default rate\n",
        "- Anambra - 101 loans, 26 defaults, 26% default rate\n",
        "- Abia - 56 loans, 16 defaults, 29% default rate\n",
        "\n",
        "**North East - 95 loans, 17 defaults, 18% default rate**\n",
        "- Gombe - 15 loans, 3 defaults, 20% default rate (exact)\n",
        "- Borno - 5 loans, 1 default, 20% default rate (exact)\n",
        "- Bauchi - 23 loans, 3 defaults, 13% default rate\n",
        "- Adamawa - 52 loans, 10 defaults, 19% default rate\n",
        "\n",
        "**Higest/Lowest:**\n",
        "- The region with the highest default rate is South South - 1330 loans, 308 defaults, 23% default rate.\n",
        "- The region with the lowest default rate is North West - 431 loans, 71 defaults, 16% default rate\n",
        "\n",
        "- The state with the highest default rate is Ebonyi - 19 loans, 6 defaults, 32% default rate.\n",
        "- The state with the lowest default rate excluding those at 0% due to the low number of loans is Niger - 71 loans, 8 defaults, 11% default rate and Sokoto - 19 loans, 2 defaults, 11% default rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a22d6c1-b39f-483b-bdee-20207ecc8baa",
      "metadata": {
        "id": "7a22d6c1-b39f-483b-bdee-20207ecc8baa"
      },
      "source": [
        "##### **Insights:**\n",
        "\n",
        "**Why Lagos/South West leads:**\n",
        "- Lagos is Nigeria's commercial capital and largest city (~15M people)\n",
        "- Most banks, fintechs, and financial services are headquartered there\n",
        "- Better internet, mobile penetration for digital lending\n",
        "- Higher population density = more potential customers\n",
        "- Generally higher disposable income supports loan demand\n",
        "\n",
        "**Regional Default Rate Patterns**\n",
        "\n",
        "**South South (23% default rate) - Highest Risk**\n",
        "- Economy heavily reliant on oil sector volatility\n",
        "- Oil spills, pollution affecting agriculture/fishing\n",
        "- Despite oil wealth, limited economic diversification\n",
        "- A state like Cross River (31%) is a border state with limited industrial base\n",
        "\n",
        "**North West (16% default rate) - Lowest Risk**\n",
        "- More stable, seasonal income from farming\n",
        "- Islamic banking principles, cautious borrowing\n",
        "- Less exposure to volatile urban employment\n",
        "- Social capital for loan repayment support\n",
        "\n",
        " **State-Level Insights**\n",
        "\n",
        "**High-Risk States:**\n",
        "- Ebonyi (32%): Predominantly rural, limited industry, relies on agriculture\n",
        "- Cross River (31%): Border state, tourism-dependent, limited diversification  \n",
        "- Katsina (29%): Despite being northern, faces security challenges affecting economy\n",
        "- Abia (29%): Industrial decline, unemployment in manufacturing sectors\n",
        "\n",
        "**Low-Risk States:**\n",
        "- Niger (11%): Federal Capital Territory proximity, government employment stability\n",
        "- Sokoto (11%): Strong Islamic financial culture, agricultural stability\n",
        "- Ondo (15%): Cocoa production, more diversified agriculture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2ae27c3-a99f-4ffd-9a5d-84e48ee7eeb8",
      "metadata": {
        "id": "c2ae27c3-a99f-4ffd-9a5d-84e48ee7eeb8"
      },
      "source": [
        "**Business Implications**\n",
        "\n",
        "**Volume vs Risk Trade-offs:**\n",
        "- Lagos: Massive volume (43% of loans) with moderate risk (20%) = profit center\n",
        "- South East: Higher risk but smaller volume = manageable with targeted strategies  \n",
        "- North: Lower risk but limited volume = expansion opportunity\n",
        "\n",
        "**Regional Strategies:**\n",
        "- South West: Focus on volume growth with current risk levels\n",
        "- South South: Implement stricter underwriting given oil volatility\n",
        "- North: Develop Sharia-compliant products, expand rural access\n",
        "- South East: Partner with local institutions for better risk assessment\n",
        "\n",
        "**Cultural Considerations:**\n",
        "- Northern states: Islamic finance principles, community-based lending\n",
        "- Southern states: More Western financial practices, individual credit focus\n",
        "- Oil states: Income volatility requires flexible repayment terms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8087cdd6-7d8c-419d-a7dc-24406fc8b475",
      "metadata": {
        "id": "8087cdd6-7d8c-419d-a7dc-24406fc8b475"
      },
      "source": [
        "#### **Final Conclusion on Previous Loans EDA:**\n",
        "The data shows that defaults aren’t random; they revolve around new borrowers, smaller loan amounts, shorter terms, smaller loan numbers and repayment history (for repeat customers). In contrast, stability, loyalty, and stronger financial profiles lower the likelihood of default. Location also has a role to play as well due to regional economic conditions, infrastructure development, and cultural financial practices.\n",
        "\n",
        "Borrower demographics and repayment behavior act as the most direct predictors, while bank policies shape these outcomes indirectly through mechanisms like risk-based interest rates and loyalty incentives. Together, these interactions create patterns that a default prediction model can capture, allowing banks not only to forecast risk with greater accuracy but also to refine their lending strategies in ways that support both growth and repayment stability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fbe97dd-447c-4b2d-9eeb-d496b71623f6",
      "metadata": {
        "id": "0fbe97dd-447c-4b2d-9eeb-d496b71623f6"
      },
      "source": [
        "### EDA - Main Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a477fcf2-49a2-4e5d-9ecc-d8fe21e6ea8c",
      "metadata": {
        "id": "a477fcf2-49a2-4e5d-9ecc-d8fe21e6ea8c"
      },
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a844547-f571-43a1-93e9-0c12de3558ae",
      "metadata": {
        "id": "9a844547-f571-43a1-93e9-0c12de3558ae"
      },
      "outputs": [],
      "source": [
        "# percentage of missing values\n",
        "df.isna().sum().sort_values(ascending=False)/len(df) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7562b845-2de2-48b1-a8ec-62b5c8e1ee49",
      "metadata": {
        "id": "7562b845-2de2-48b1-a8ec-62b5c8e1ee49"
      },
      "outputs": [],
      "source": [
        "# loan months\n",
        "df.loan_month.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdeaec31-1c4d-4bde-b0db-88d43d6f5789",
      "metadata": {
        "id": "cdeaec31-1c4d-4bde-b0db-88d43d6f5789"
      },
      "outputs": [],
      "source": [
        "# loan year\n",
        "df.loan_year.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a90e20d-9011-4acb-8e18-03997cdb19a1",
      "metadata": {
        "id": "0a90e20d-9011-4acb-8e18-03997cdb19a1"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbff63f2-4ef5-45cc-9f4d-87e80866ede3",
      "metadata": {
        "id": "dbff63f2-4ef5-45cc-9f4d-87e80866ede3"
      },
      "source": [
        "#### **Observations:**\n",
        "- There are approximately 25% of customers missing all their demographic information due to there not being a match for their customerid in the demographics dataframe. The 36% missing in the employment_status_clients column includes those without a match as well as those that were missing in the original demographics dataframe\n",
        "- There are approximately 0.2% of customers missing previous loans information\n",
        "- The counts and age are of the wrong data type - float, it should be integer\n",
        "- The termdays should be categorical for better interpretability\n",
        "- There is no distinct difference between customers with missing demographic information and those without missing demographic information\n",
        "- All loans are from the year 2017\n",
        "- All loans are from the month of July\n",
        "\n",
        "#### **Insights:**\n",
        "- 1095 customers (approximately 25%) are missing all demographic data\n",
        "- Considering that:\n",
        "  - all loans are from July, 2017,\n",
        "  - there are other customers with their full demographic information,\n",
        "  - all the customers have taken a loan before,\n",
        "  - and a loan can't be taken without submitting your birth date. It can be concluded that the missingness is most likely due to bank issues e.g data transfer/storage. This can be best described as MAR (Missing At Random) since the missing value can be tied to an observed variable.\n",
        "-  The rows of the customers missing previous loans information can be dropped\n",
        "\n",
        "#### **To Do:**\n",
        "- Create a copy of the main dataframe with missing values as imputing values will create a bias and disrupt the integrity of the analysis\n",
        "- Change incorrect data types\n",
        "- Perform EDA on clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1dfd4c8-4d1b-4e4a-985d-7803c48dd106",
      "metadata": {
        "id": "b1dfd4c8-4d1b-4e4a-985d-7803c48dd106"
      },
      "outputs": [],
      "source": [
        "#eda was also done on the full data to ensure correct values for categorical data, summary statistics etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4a0ba8-19cc-4a7c-b7f4-2beeb6cbf343",
      "metadata": {
        "id": "0c4a0ba8-19cc-4a7c-b7f4-2beeb6cbf343"
      },
      "outputs": [],
      "source": [
        "# analysis dataframe\n",
        "df1 = df.copy()\n",
        "df1 = df1.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918d3cfb-cef6-401d-84ee-c23dfdea1145",
      "metadata": {
        "id": "918d3cfb-cef6-401d-84ee-c23dfdea1145"
      },
      "outputs": [],
      "source": [
        "df1.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3353fa92-d843-4044-8c10-9a14b052bec5",
      "metadata": {
        "id": "3353fa92-d843-4044-8c10-9a14b052bec5"
      },
      "outputs": [],
      "source": [
        "# change data types\n",
        "dtype_cols = ['default_count', 'no_default_count', 'early_count', 'on_time_count', 'prev_loan_number', 'approval_speed_hours', 'age']\n",
        "for col in dtype_cols:\n",
        "    df1[col] = df1[col].astype(int)\n",
        "\n",
        "df1['termdays'] = df1['termdays'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496aa67e-7d52-4db1-926f-ce5145440260",
      "metadata": {
        "id": "496aa67e-7d52-4db1-926f-ce5145440260"
      },
      "outputs": [],
      "source": [
        "# split into numerical, categorical and geographical columns\n",
        "num_cols = ['loannumber', 'loanamount', 'totaldue', 'default_count', 'no_default_count', 'early_count', 'on_time_count', 'prev_loan_number',\n",
        "            'default_rate', 'prev_total_loan', 'prev_max_loan_amount', 'approval_speed_hours', 'approval_hour', 'interest_amount', 'interest_rate', 'age']\n",
        "\n",
        "cat_cols = ['termdays', 'bank_account_type', 'bank_name_clients', 'employment_status_clients', 'approval_day',\n",
        "            'loan_amount_category', 'loan_number_category', 'age_group']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd53dbec-ae56-4f55-a1a8-091cf636b078",
      "metadata": {
        "id": "cd53dbec-ae56-4f55-a1a8-091cf636b078"
      },
      "source": [
        "#### Numerical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baf0076a-c374-4516-b2b2-9402567e7707",
      "metadata": {
        "id": "baf0076a-c374-4516-b2b2-9402567e7707",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# summary statistics\n",
        "df1[num_cols].describe().round(2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff901c3-2071-456e-9c93-e827b5a17e38",
      "metadata": {
        "id": "bff901c3-2071-456e-9c93-e827b5a17e38"
      },
      "source": [
        "##### **Insights**:\n",
        "In the given dataset:\n",
        "- The minimum number of loans a customer has taken is 2, the maximum is 27. This suggests that all the customers have taken a loan before\n",
        "- The minimum loan amount is 10000.0, the maximum is 60000.0\n",
        "- The minimum default count is 0, the maximum is 11 (full data).\n",
        "- The minimum no default count is 0, the maximum is 26\n",
        "- The minimum number of loans previously taken by a customer is 1, the maximum is 26\n",
        "- The minimum default rate is 0.00%, the maximum is 1.00%\n",
        "- The minimum as well as the most common/average approval speed is 1 hour, the maximum is 19 hours\n",
        "- The minimum approval hour is 00:00(12am), the maximum is 23:00(11pm).\n",
        "- The minimum interest amount added to a loan is 0.0, the maximum is 9000.0\n",
        "- The minimum interest rate is 0.0 percent, the maximum is 0.23 percent\n",
        "- The minimum age is 21 years, the maximum is 56 years\n",
        "\n",
        "This was also done on the full data to get the actual values, 'full data' was used to show that though the max on the summary statistics of default count is 9 on the clean data, the actual max is 11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4e70de-a5c8-479f-a2bb-67c0b1efad56",
      "metadata": {
        "id": "7e4e70de-a5c8-479f-a2bb-67c0b1efad56"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "df1[df1['interest_amount'] == 0.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2071558c-a6d9-4e31-a120-694fc35433ef",
      "metadata": {
        "id": "2071558c-a6d9-4e31-a120-694fc35433ef"
      },
      "source": [
        "This is the customer's 12th loan and the waive in interest can be attributed to the customer defaulting on only 1 out of 11 past loans. 7 payments were early and 3 were on time. This correlates with the findings in the previous loans EDA. The customer has a permanent employment status which had the 4th lowest default rate of all 6 in the category and is in the adult age group which had the 3rd lowest default of all 4 in the category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca21dd6-ba68-40e5-ae24-b6bd7af157cb",
      "metadata": {
        "id": "0ca21dd6-ba68-40e5-ae24-b6bd7af157cb"
      },
      "outputs": [],
      "source": [
        "# correlation matrix\n",
        "corr_matrix = df1[num_cols].corr()\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='Blues', fmt='.3f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a62bfb0-81f1-4eef-8384-21aa44f47fb8",
      "metadata": {
        "id": "1a62bfb0-81f1-4eef-8384-21aa44f47fb8"
      },
      "source": [
        "##### **Top 3 Strongest Positive Correlations (as one increases, the other increases):**\n",
        "1. totaldue and loanamount = 0.994: Larger loan amounts result in higher total due\n",
        "2. no_default_count and early_count = 0.977: Borrowers who never or hardly default often repay early, showing strong reliability.\n",
        "3. loannumber/prev_loan_number and prev_total_loan = 0.946: Customers with more loan cycles typically accumulate higher past loan amounts, reflecting loyalty and borrowing capacity.\n",
        "\n",
        "##### **Top 3 Strongest Negative Correlations (as one increases, the other decreases):**\n",
        "1. no_default_count and interest_rate = -0.583: The more defaults avoided, the lower the interest rate offered.\n",
        "2. loannumber/prev_loan_number and interest_rate = -0.571: Repeat borrowers tend to secure lower rates.\n",
        "3. prev_total_loan and interest_rate = -0.569: Borrowers with high loan numbers tend to have reduced interest rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ffcbf57-4a06-4885-8b81-9ef8aac78d6f",
      "metadata": {
        "id": "9ffcbf57-4a06-4885-8b81-9ef8aac78d6f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# num_cols distribution and boxplot - - loannumber, loanamount, age and approval speed\n",
        "num_cols = ['loannumber', 'loanamount', 'approval_speed_hours', 'age']\n",
        "for col in num_cols:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    # histogram\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df1[col], bins=30, kde=True)\n",
        "    plt.title(f\"{col.capitalize()} Distribution\")\n",
        "    plt.xlabel(col.capitalize())\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    #boxplot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(x=df1[col], color='green')\n",
        "    plt.title(f\"{col.capitalize()} Boxplot\")\n",
        "    plt.xlabel(col.capitalize())\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb88c4c0-ca45-4d52-aee7-e5e607574da8",
      "metadata": {
        "id": "fb88c4c0-ca45-4d52-aee7-e5e607574da8"
      },
      "source": [
        "##### **Insights:**\n",
        "- Most customers are on their 2nd loan\n",
        "- Most customers took a loan amount of 10000\n",
        "- Bank usually take 1 hour to approve loans after creation\n",
        "- The customers are majorly in their 30's"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "395de47b-c017-45ab-8e87-5b34b814b37a",
      "metadata": {
        "id": "395de47b-c017-45ab-8e87-5b34b814b37a"
      },
      "source": [
        "#### Categorical Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf6f19e4-3696-476c-aff4-dc69cba1e370",
      "metadata": {
        "id": "bf6f19e4-3696-476c-aff4-dc69cba1e370",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# cat cols visualization\n",
        "fig, axes = plt.subplots(4, 2, figsize=(12, 15))\n",
        "\n",
        "for i, col in enumerate(cat_cols):\n",
        "    ax = axes[i//2, i%2]\n",
        "    sns.countplot(data=df1, x=col, ax=ax)\n",
        "    ax.set_title(f\"{col.capitalize()} Countplot\")\n",
        "    ax.set_xlabel(col.capitalize())\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.tick_params(axis='x', rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c06cc47-3bd0-4f79-aaea-4e1d5f8dc84d",
      "metadata": {
        "id": "1c06cc47-3bd0-4f79-aaea-4e1d5f8dc84d"
      },
      "source": [
        "##### **Insights:**\n",
        "- The most common term of a loan is 30 days. The least common term of a loan is 90 days (higher term = higher loan amount = higher loan number)\n",
        "- The most common account type is a Savings account. The least common account type is a Current account\n",
        "- GT Bank has the highest number of loan clients. Unity Bank has the lowest number of loan clients\n",
        "- The employment status with the most clients is 'Permanent'. The employment status with the least clients is 'Contract'\n",
        "- The day with the most loan activity is Wednesday (full data). The day with the least loan activity is Sunday\n",
        "- Customers tend to take more than one loan, with most having a small number of loans taken (0 - 5)\n",
        "- Customers take smaller loan amounts (0 - 10000.0).\n",
        "- Customers were mostly Adults (26 - 40 years)\n",
        "\n",
        "This was also done on the full data to get the actual values, 'full data' is used to show that though the day with the most activity shows Monday, it is actually Wednesday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb6a73a-62f1-4a64-8775-78fdeb0167aa",
      "metadata": {
        "id": "acb6a73a-62f1-4a64-8775-78fdeb0167aa"
      },
      "outputs": [],
      "source": [
        "# target column imbalance\n",
        "sns.countplot(data=df, x='good_bad_flag')\n",
        "plt.title(f\"Target Countplot\")\n",
        "plt.xlabel('Flag')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2df9a5-26b1-4890-9432-718b2450ac9f",
      "metadata": {
        "id": "cf2df9a5-26b1-4890-9432-718b2450ac9f"
      },
      "outputs": [],
      "source": [
        "df.good_bad_flag.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc5f65fb-a0a0-44cc-a460-c6b22f074ac6",
      "metadata": {
        "id": "bc5f65fb-a0a0-44cc-a460-c6b22f074ac6"
      },
      "outputs": [],
      "source": [
        "df.good_bad_flag.value_counts(normalize=True) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "032d8aee-e162-4ffd-a60e-7d38fbe6d3b4",
      "metadata": {
        "id": "032d8aee-e162-4ffd-a60e-7d38fbe6d3b4"
      },
      "source": [
        "The dataset shows a significant class imbalance with 78.2% (3416) good flags versus 21.8% (952) bad flags. This imbalance would require careful handling in predictive modeling efforts through techniques like SMOTE, class weighting, or stratified sampling as it will cause a bias towards the majority class (in this case, good flags)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ae7e785-9841-4df0-9dde-8d34101fb4e6",
      "metadata": {
        "id": "8ae7e785-9841-4df0-9dde-8d34101fb4e6"
      },
      "source": [
        "#### **Final Conclusion on Main Dataframe EDA:**\n",
        "1. **Key Findings and Validation:**\n",
        "\n",
        "With only 4,368 rows in this dataset, the size is too small for a detailed analysis such as default rate analysis across multiple features or geographical analysis as we did in previous loans. Despite these limitations, I was able to extract meaningful insights by leveraging knowledge from previous loan analysis and conducting comprehensive correlation and distribution analysis. The current analysis confirms and builds upon the following critical insights from previous loan data:\n",
        "- Loyalty Based Pricing Mechanism: The dataset validates behavioral incentives, showing that interest rates decrease for consistent repayers (correlation = -0.583 between no_default_count and interest_rate)\n",
        "- Loan Cycle Benefits: Repeat borrowers (higher loan numbers) consistently secure lower interest rates (correlation = -0.571), confirming the reward system for customer loyalty\n",
        "- Risk Segmentation: Previous findings about high-risk demographics (young adults, unemployed individuals) and seasonal patterns (September defaults) are reinforced\n",
        "\n",
        "2. **Operational Insights:**\n",
        "- Loan Processing Efficiency: Most loans are approved within 1 hour, indicating streamlined operations\n",
        "- Customer Demographics: Primarily adults aged 26-40 with permanent employment and savings accounts through GT Bank"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e69ffcd-ada6-4166-8766-5694f44e1be0",
      "metadata": {
        "id": "8e69ffcd-ada6-4166-8766-5694f44e1be0"
      },
      "source": [
        "#### **Recommendations**\n",
        "Given the dataset limitations, recommendations focus on operational improvements leveraging proven loyalty and behavioral patterns:\n",
        "\n",
        "1. **Data-Driven Risk Management:**\n",
        "   - Loyalty Based Pricing: Formalize the observed pattern by reducing interest rates after successful loan milestones (4th, 5th loans) for consistent repayers while maintaining flat rates for customers with default history\n",
        "   - Multi-factor Assessment: Combine loan history + repayment behavior + geographic risk + demographic factors (age, employment, account type) for comprehensive risk evaluation\n",
        "   - Data Collection Enhancement: Expand data collection efforts to build a more robust dataset (target 10,000+ entries) for reliable statistical analysis and predictive modeling\n",
        "\n",
        "2. **Operational Implementation:**\n",
        "   - Automated Systems: Build loyalty tracking systems to automatically adjust rates and limits based on successful loan completion milestones\n",
        "   - Real-time Monitoring: Implement tracking of validated risk factors (age groups, employment status, seasonal patterns) identified in previous analysis\n",
        "   - Seasonal Adjustments: Prepare for September default spikes by implementing additional screening during August loan originations\n",
        "\n",
        "3. **Strategic Portfolio Management:**\n",
        "   - Predictive Scoring: Use identified patterns (new borrowers + small amounts + short terms + volatile regions) for proactive default prevention\n",
        "   - Segment Focus: Leverage confirmed low-risk segments (seniors, contract workers, established customers) while carefully managing exposure to high-risk categories\n",
        "   - Future Model Development: Once larger datasets are available, develop integrated predictive models incorporating all behavioral and demographic insights\n",
        "\n",
        "While the current dataset of 4,368 rows provides valuable directional insights and confirms previously identified patterns, it falls short of the statistical power needed for robust predictive modeling or categorical analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c833d9e1-9cea-44ec-b0f1-b271e85bafcc",
      "metadata": {
        "id": "c833d9e1-9cea-44ec-b0f1-b271e85bafcc"
      },
      "source": [
        "#### **Predictive Modeling Approach**\n",
        "\n",
        "Despite the dataset size limitations, I will proceed with building a predictive model as part of this project. The model development will focus on:\n",
        "-  Class Imbalance Handling: Address the 78.2% vs 21.8% target distribution using techniques like SMOTE, class weighting, or stratified sampling\n",
        "- Model Tuning: Implement strategies to improve model performance\n",
        "\n",
        "Expected Outcomes:\n",
        "- The model will help predict loan defaults\n",
        "- Performance metrics will be interpreted cautiously due to dataset size constraints\n",
        "- Results will serve as a foundation for future model enhancement with larger datasets\n",
        "\n",
        "The current analysis provides a solid foundation for model features, and while prediction accuracy may be limited by sample size, the model will demonstrate the methodology and framework for larger scale implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c107d2c-5431-4832-a0fa-69548407ef7c",
      "metadata": {
        "id": "1c107d2c-5431-4832-a0fa-69548407ef7c"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a0808c-234a-4336-9589-7f03d2b85fc5",
      "metadata": {
        "id": "d4a0808c-234a-4336-9589-7f03d2b85fc5"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9978c195-6ee6-4107-bb12-1eee87cc4f9c",
      "metadata": {
        "id": "9978c195-6ee6-4107-bb12-1eee87cc4f9c"
      },
      "outputs": [],
      "source": [
        "# drop columns that won't be useful for prediction\n",
        "df = df.drop(['customerid', 'systemloanid', 'totaldue', 'longitude_gps', 'latitude_gps', 'loan_year', 'loan_month'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# missing values\n",
        "(df.isna().sum().sort_values(ascending=False)/len(df))*100"
      ],
      "metadata": {
        "id": "dni0exxeLdVH"
      },
      "id": "dni0exxeLdVH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c660b72-3eb0-43c1-8dfb-33d09e9ba220",
      "metadata": {
        "id": "3c660b72-3eb0-43c1-8dfb-33d09e9ba220"
      },
      "outputs": [],
      "source": [
        "# drop rows with less than 5% missing values\n",
        "df = df.dropna(subset=['default_count', 'no_default_count', 'early_count',\n",
        "                       'on_time_count', 'prev_loan_number', 'default_rate', 'prev_total_loan',\n",
        "                       'prev_max_loan_amount'], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data types\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "bAdmCFkOLg8G"
      },
      "id": "bAdmCFkOLg8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b59db80d-8ab4-4e93-9353-39fa325222e2",
      "metadata": {
        "id": "b59db80d-8ab4-4e93-9353-39fa325222e2"
      },
      "outputs": [],
      "source": [
        "# change data types\n",
        "dtype_cols = ['default_count', 'no_default_count', 'early_count', 'on_time_count', 'prev_loan_number', 'approval_speed_hours']\n",
        "for col in dtype_cols:\n",
        "    df[col] = df[col].astype(int)\n",
        "\n",
        "df['termdays'] = df['termdays'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b687e94-7bc7-4348-83cf-622e7bd3b857",
      "metadata": {
        "id": "9b687e94-7bc7-4348-83cf-622e7bd3b857"
      },
      "outputs": [],
      "source": [
        "# encode target\n",
        "df['good_bad_flag'] = np.where(df['good_bad_flag'] == 'Good', 1, 0) # Good = 1, Bad = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17352b60-df7d-42bf-9852-7530bea9707c",
      "metadata": {
        "id": "17352b60-df7d-42bf-9852-7530bea9707c"
      },
      "outputs": [],
      "source": [
        "# add missingness indicator - could be a useful feature\n",
        "null_cols = [col for col in df if df[col].isna().sum() > 0]\n",
        "null_cols\n",
        "for col in null_cols:\n",
        "    df[f\"{col}_is_missing\"] = np.where(df[col].isna(), 1, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc68c38b-37c1-4dcb-8541-94e8791f6779",
      "metadata": {
        "id": "cc68c38b-37c1-4dcb-8541-94e8791f6779"
      },
      "outputs": [],
      "source": [
        "# split data into features (X) and target(y)\n",
        "X = df.drop('good_bad_flag', axis=1)\n",
        "y = df['good_bad_flag']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a15136-b590-47ef-8291-b1fc507601e7",
      "metadata": {
        "id": "68a15136-b590-47ef-8291-b1fc507601e7"
      },
      "outputs": [],
      "source": [
        "# split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.08, stratify=y, random_state=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test size was reduced to improve model performance"
      ],
      "metadata": {
        "id": "HtH78JOGpc1A"
      },
      "id": "HtH78JOGpc1A"
    },
    {
      "cell_type": "markdown",
      "id": "3b5ac774-a8dc-42db-a2a7-f191dd31dd30",
      "metadata": {
        "id": "3b5ac774-a8dc-42db-a2a7-f191dd31dd30"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ce935e-74c2-4d07-a8b2-597a0f50dca1",
      "metadata": {
        "id": "b1ce935e-74c2-4d07-a8b2-597a0f50dca1"
      },
      "source": [
        "- Handle missing values\n",
        "- Feature selection\n",
        "- Scaling and encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8acf7899-3f88-4015-88de-941044ce7525",
      "metadata": {
        "id": "8acf7899-3f88-4015-88de-941044ce7525"
      },
      "source": [
        "### Handling Missing Values\n",
        "Creating imputers for missing values using model prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3819598-b766-4975-9d23-6868e8ece09e",
      "metadata": {
        "id": "f3819598-b766-4975-9d23-6868e8ece09e"
      },
      "outputs": [],
      "source": [
        "# create functions for preprocessing and prediction\n",
        "\n",
        "# preprocess data\n",
        "def preprocess(data):\n",
        "    #scale\n",
        "    for num_col in data.select_dtypes(include=np.number):\n",
        "        if num_col == 'is_na':\n",
        "            pass\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "            data[num_col] = scaler.fit_transform(data[[num_col]])\n",
        "    #encode\n",
        "    for cat_col in data.select_dtypes(exclude=np.number):\n",
        "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        encoded = encoder.fit_transform(data[[cat_col]])\n",
        "        encoded = pd.DataFrame(encoded, index=data.index, columns=encoder.get_feature_names_out([cat_col]))\n",
        "        data = data.drop(cat_col, axis=1).join(encoded)\n",
        "    return data\n",
        "\n",
        "# split data\n",
        "def split(data, col):\n",
        "    # split into train and test\n",
        "    train = data[data['is_na'] == 0]\n",
        "    test = data[data['is_na'] == 1]\n",
        "    # X_train, y_train, X_test\n",
        "    X_train = train.drop([col, 'is_na'], axis=1)\n",
        "    y_train = train[col]\n",
        "    X_test = test.drop([col, 'is_na'], axis=1)\n",
        "    return X_train, y_train, X_test\n",
        "\n",
        "# predict missing values\n",
        "def train_predict_cat(X_train, y_train, X_test):\n",
        "    model = RandomForestClassifier(random_state=20)\n",
        "    # fit and predict\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "def train_predict_num(X_train, y_train, X_test):\n",
        "    model = RandomForestRegressor(random_state=20)\n",
        "    # fit and predict\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84d7aca0-58c5-4c34-ac24-72941a5e64b6",
      "metadata": {
        "id": "84d7aca0-58c5-4c34-ac24-72941a5e64b6"
      },
      "outputs": [],
      "source": [
        "# create categorical imputer\n",
        "class CatImputer(TransformerMixin, BaseEstimator):\n",
        "    #fit\n",
        "    def fit(self, X, y=None):\n",
        "        self.feature_names_in_ = X.columns.tolist()\n",
        "        return self\n",
        "    #transform - impute\n",
        "    def transform(self, X):\n",
        "        df = X.copy().reset_index(drop=True)\n",
        "\n",
        "        # get categorical columns\n",
        "        cat_cols = df.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "        for col in cat_cols:\n",
        "            # get index of missing values\n",
        "            nan_mask = df[col].isna()\n",
        "            if not nan_mask.any():\n",
        "                    continue\n",
        "\n",
        "            # add column to indicate missing\n",
        "            df['is_na'] = nan_mask.astype(int)\n",
        "\n",
        "            # preprocess\n",
        "            X_temp = df.drop([col], axis=1)\n",
        "            y_temp = df[col]\n",
        "            X_temp = preprocess(X_temp)\n",
        "            data = X_temp.join(y_temp)\n",
        "\n",
        "            # X_train, y_train, X_test\n",
        "            X_train, y_train, X_test = split(data, col)\n",
        "\n",
        "            if len(X_train) > 0 and len(X_test) > 0:\n",
        "                # train model and predict\n",
        "                y_pred = train_predict_cat(X_train, y_train, X_test)\n",
        "\n",
        "                # impute\n",
        "                df.loc[nan_mask, col] = y_pred\n",
        "\n",
        "            df.drop(columns='is_na', inplace=True)\n",
        "        return df\n",
        "\n",
        "        def get_feature_names_out(self, input_feat = None):\n",
        "            if input_feat is None:\n",
        "                return np.array(self.feature_names_in_)\n",
        "            else:\n",
        "                return np.array(input_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5e8842a-99af-4207-9225-dbcc8e31c48b",
      "metadata": {
        "id": "c5e8842a-99af-4207-9225-dbcc8e31c48b"
      },
      "outputs": [],
      "source": [
        "# create numerical imputer\n",
        "class NumImputer(TransformerMixin, BaseEstimator):\n",
        "    #fit\n",
        "    def fit(self, X, y=None):\n",
        "        self.feature_names_in_ = X.columns.tolist()\n",
        "        return self\n",
        "\n",
        "    #transform - impute\n",
        "    def transform(self, X):\n",
        "        df = X.copy().reset_index(drop=True)\n",
        "\n",
        "        num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "        for col in num_cols:\n",
        "            # get index of missing values\n",
        "            nan_mask = df[col].isna()\n",
        "            if not nan_mask.any():\n",
        "                    continue\n",
        "\n",
        "            # add column to indicate missing\n",
        "            df['is_na'] = nan_mask.astype(int)\n",
        "\n",
        "            # preprocess\n",
        "            X_temp = df.drop([col], axis=1)\n",
        "            y_temp = df[col]\n",
        "            X_temp = preprocess(X_temp)\n",
        "            data = X_temp.join(y_temp)\n",
        "\n",
        "            # X_train, y_train, X_test\n",
        "            X_train, y_train, X_test = split(data, col)\n",
        "\n",
        "            if len(X_train) > 0 and len(X_test) > 0:\n",
        "                # train model and predict\n",
        "                y_pred = train_predict_num(X_train, y_train, X_test)\n",
        "\n",
        "                # impute\n",
        "                df.loc[nan_mask, col] = y_pred\n",
        "\n",
        "            df.drop(columns='is_na', inplace=True)\n",
        "        return df\n",
        "\n",
        "        def get_feature_names_out(self, input_feat = None):\n",
        "            if input_feat is None:\n",
        "                return np.array(self.feature_names_in_)\n",
        "            else:\n",
        "                return np.array(input_feat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1453239e-a533-48bf-a1da-f7d67372ad11",
      "metadata": {
        "id": "1453239e-a533-48bf-a1da-f7d67372ad11"
      },
      "source": [
        "I created custom imputers for the numerical and categorical columns with model prediction. Random forest was used as the prediction model.\n",
        "\n",
        "In the imputers, the fit function returns self because there is nothing the imputer is learning from.\n",
        "\n",
        "In the transform function, the imputing process takes place. The data goes through preprocessing steps where it uses the data with no missing values as features and those with missing values as the target. The model is fit on the training data and is then used to predicted the missing values. The missing values are imputed back to the dataframe using the index of the missing values gotten at the start.\n",
        "\n",
        "Each imputer wrapped in a class and inherits functions from the super classes TransformerMixin and BaseEstimator to make the function work as an imputer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1be0baa-8ea7-483f-a2f1-d03127d5e555",
      "metadata": {
        "id": "b1be0baa-8ea7-483f-a2f1-d03127d5e555"
      },
      "source": [
        "### Base Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b991e5ab-1ebd-4e65-9649-c2f8fdf56f2e",
      "metadata": {
        "id": "b991e5ab-1ebd-4e65-9649-c2f8fdf56f2e"
      },
      "outputs": [],
      "source": [
        "# creating copies to avoid altering the main data during testing scenarios\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25fe5ba0-4093-4e62-b1dd-ab35aae9ecce",
      "metadata": {
        "id": "25fe5ba0-4093-4e62-b1dd-ab35aae9ecce"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "num_transformer = Pipeline(steps=[\n",
        "    ('num_imputer', NumImputer()),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    ('cat_imputer', CatImputer()),\n",
        "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', num_transformer, num_cols),\n",
        "        ('cat', cat_transformer, cat_cols),\n",
        "\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d4b75ee-ec9d-4c49-b183-b3d463e7e4c5",
      "metadata": {
        "id": "6d4b75ee-ec9d-4c49-b183-b3d463e7e4c5"
      },
      "outputs": [],
      "source": [
        "# create a function to predict and evaluate\n",
        "def predict_evaluate(X_train, X_test, y_train, y_test, model):\n",
        "    num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    # preprocessor\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', num_transformer, num_cols),\n",
        "        ('cat', cat_transformer, cat_cols),\n",
        "\n",
        "    ])\n",
        "\n",
        "    #pipeline\n",
        "    model_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    #fit\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # predict\n",
        "    train_pred = model_pipeline.predict(X_train)\n",
        "    test_pred = model_pipeline.predict(X_test)\n",
        "\n",
        "    #accuracy\n",
        "    train_acc = accuracy_score(y_train, train_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\")\n",
        "    print(classification_report(y_test, test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35eb44f4-2b75-44b6-8cfa-3f806dd5113d",
      "metadata": {
        "id": "35eb44f4-2b75-44b6-8cfa-3f806dd5113d"
      },
      "outputs": [],
      "source": [
        "# using logistic regression as a base model\n",
        "model = LogisticRegression(random_state=20, max_iter=1000, solver='liblinear')\n",
        "predict_evaluate(X_train_1, X_test_1, y_train_1, y_test_1, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "515945cf-bfb5-4deb-acb7-2635e9c05d2c",
      "metadata": {
        "id": "515945cf-bfb5-4deb-acb7-2635e9c05d2c"
      },
      "source": [
        "Considering that the overall goal of the model is to be able to detect possible defaulters, the metric I will be focusing on is recall. Recall of the 0 class (bad flags) gives the performance of the model's ability to detect defaulters. Right now with a recall of 0.11 on the 0 class, the model can only detect 11% bad flags which is very poor.\n",
        "\n",
        "The goal is to improve the recall score of the model. Using the baseline model (Logistic Regression), I will do this through:\n",
        "- Handling class imbalance\n",
        "- Feature selection\n",
        "\n",
        "Then I will test the data on 5 or more models and choose the one with the best recall score of the 0 class (bad flags). When choosing, I will also consider the precision ensuring that there is a good balance between catching most of the bad loans and not incorrectly flagging too many good ones. Then I will perform futher tuning on the chosen model.\n",
        "\n",
        "Due to the size of the dataset, however, the model has very little to learn from and performance might not be very high."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36010eda-2729-47ff-88a6-9066fc18175c",
      "metadata": {
        "id": "36010eda-2729-47ff-88a6-9066fc18175c"
      },
      "source": [
        "### Handle Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63c8679-4f21-44a3-bfde-0ed568e1d1b2",
      "metadata": {
        "id": "b63c8679-4f21-44a3-bfde-0ed568e1d1b2"
      },
      "outputs": [],
      "source": [
        "# ADASYN\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "adasyn = ADASYN(random_state=20)\n",
        "\n",
        "X_adn, y_adn = adasyn.fit_resample(X_train_processed, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b34191-7631-4ecf-b41c-3fd1bb28ae4c",
      "metadata": {
        "id": "84b34191-7631-4ecf-b41c-3fd1bb28ae4c"
      },
      "outputs": [],
      "source": [
        "y_adn.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c21c763-77ba-4dea-86b1-e61668ee7073",
      "metadata": {
        "id": "0c21c763-77ba-4dea-86b1-e61668ee7073"
      },
      "outputs": [],
      "source": [
        "# SMOTE\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "smote = SMOTE(random_state=20)\n",
        "\n",
        "X_smote, y_smote = smote.fit_resample(X_train_processed, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e811003-6bac-4797-80fa-d8366ef6e7ea",
      "metadata": {
        "id": "7e811003-6bac-4797-80fa-d8366ef6e7ea"
      },
      "outputs": [],
      "source": [
        "y_smote.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc04ff0-5fe1-44b0-b604-045ee13b85ca",
      "metadata": {
        "id": "cdc04ff0-5fe1-44b0-b604-045ee13b85ca"
      },
      "outputs": [],
      "source": [
        "def predict_evaluate_over(X_train_resampled, X_test, y_train_resampled, y_test, model):\n",
        "    #preprocess\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    #fit\n",
        "    model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "    # predict\n",
        "    train_pred = model.predict(X_train_resampled)\n",
        "    test_pred = model.predict(X_test_processed)\n",
        "\n",
        "    #accuracy\n",
        "    train_acc = accuracy_score(y_train_resampled, train_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\")\n",
        "    print(classification_report(y_test, test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83f00b18-463f-4910-a96b-5c2472714e30",
      "metadata": {
        "id": "83f00b18-463f-4910-a96b-5c2472714e30"
      },
      "outputs": [],
      "source": [
        "# ADASYN\n",
        "model = LogisticRegression(random_state=20, max_iter=1000)\n",
        "predict_evaluate_over(X_adn, X_test_1, y_adn, y_test_1, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728c5cb8-7897-434b-a35b-eaf7704e0d54",
      "metadata": {
        "id": "728c5cb8-7897-434b-a35b-eaf7704e0d54"
      },
      "outputs": [],
      "source": [
        "# SMOTE\n",
        "model = LogisticRegression(random_state=20, max_iter=1000)\n",
        "predict_evaluate_over(X_smote, X_test_1, y_smote, y_test_1, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d0dbf8-a006-48bb-87d0-8d2ce4a30b77",
      "metadata": {
        "id": "50d0dbf8-a006-48bb-87d0-8d2ce4a30b77"
      },
      "source": [
        "ADASYN gave a better recall score and precison-recall trade off with a precision of 24% and a recall of 62%. (SMOTE: precision 25%, recall 57%)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Rationale for Feature Selection After Handling Imbalance**\n",
        "\n",
        "After applying ADASYN resampling to address class imbalance, the model with all features showed improved recall on the training data. However, I will proceed with systematic feature selection for several strategic reasons:\n",
        "\n",
        "1. Overfitting Risk: With synthetic data on a relatively small dataset (4,368 samples), using all features could cause the model to learn artificial patterns from the oversampled data rather than genuine predictive signals.\n",
        "\n",
        "2. Model Generalization: Feature selection helps ensure consistent performance on real-world loan applications that don’t contain synthetic data characteristics.\n",
        "\n",
        "3. Business Requirements: Fewer, more interpretable features are critical for lending decisions, regulatory compliance, and stakeholder understanding.\n",
        "\n",
        "4. Production Readiness: Removing redundant or irrelevant features creates a more stable model that’s less sensitive to variations and easier to maintain operationally.\n",
        "\n",
        "Approach: Instead of optimizing purely for training recall, I will prioritise building a robust, generalizable model. This means accepting some trade-offs in initial recall in favor of long-term reliability and business practicality."
      ],
      "metadata": {
        "id": "8sQfW36sP8Sj"
      },
      "id": "8sQfW36sP8Sj"
    },
    {
      "cell_type": "markdown",
      "id": "381e9de5-3f8e-4b0f-84d1-94653430fad0",
      "metadata": {
        "id": "381e9de5-3f8e-4b0f-84d1-94653430fad0"
      },
      "source": [
        "### Feature Selection\n",
        "- VIF\n",
        "- Mutual Information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c362b3a-cd4b-475f-bf66-aa7709faab54",
      "metadata": {
        "id": "3c362b3a-cd4b-475f-bf66-aa7709faab54"
      },
      "source": [
        "#### Variance Inflation Factor (VIF)\n",
        "Check multicolinearity of chosen columns using VIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3964a6c-cc66-46fb-bf67-fc1b4e82964d",
      "metadata": {
        "id": "a3964a6c-cc66-46fb-bf67-fc1b4e82964d"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "def mini_preprocess(X_train):\n",
        "    num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    for col in X_train.select_dtypes([\"category\"]).columns:\n",
        "        X_train[col] = X_train[col].astype(str)\n",
        "\n",
        "    X_train[num_cols] = X_train[num_cols].fillna(0)\n",
        "    X_train[cat_cols] = X_train[cat_cols].fillna(\"Missing\")\n",
        "\n",
        "    for col in cat_cols:\n",
        "        codes, unique = pd.factorize(X_train[col])\n",
        "        X_train[col] = codes\n",
        "\n",
        "    X_train = X_train.fillna(0)\n",
        "\n",
        "    return X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3184326-e3fa-456b-a21c-4a9e8c2a024d",
      "metadata": {
        "id": "d3184326-e3fa-456b-a21c-4a9e8c2a024d"
      },
      "outputs": [],
      "source": [
        "X_train_1 = mini_preprocess(X_train_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852e30bf-1630-444c-b9f7-17dee22cb74d",
      "metadata": {
        "id": "852e30bf-1630-444c-b9f7-17dee22cb74d"
      },
      "outputs": [],
      "source": [
        "def calculate_vif(data):\n",
        "    vif=pd.DataFrame()\n",
        "    vif['features'] = data.columns\n",
        "    vif['VIF_Value'] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
        "\n",
        "    return vif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c06f6c0b-d35d-43d1-abaa-04e41b98ecef",
      "metadata": {
        "id": "c06f6c0b-d35d-43d1-abaa-04e41b98ecef"
      },
      "outputs": [],
      "source": [
        "calculate_vif(X_train_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5878da6d-af51-49f0-80e7-b0d955eb76ca",
      "metadata": {
        "id": "5878da6d-af51-49f0-80e7-b0d955eb76ca"
      },
      "outputs": [],
      "source": [
        "X_train_1_vred = X_train_1.drop(['age_is_missing', 'age_group_is_missing', 'bank_account_type_is_missing', 'bank_name_clients_is_missing',\n",
        "                                'no_default_count', 'early_count', 'on_time_count', 'prev_loan_number', 'interest_amount', 'loannumber',\n",
        "                                'prev_max_loan_amount', 'interest_rate', 'loanamount', 'approval_hour', 'approval_speed_hours'], axis=1)\n",
        "cols_drop = ['age_is_missing', 'age_group_is_missing', 'bank_account_type_is_missing', 'bank_name_clients_is_missing', 'no_default_count',\n",
        "             'early_count', 'on_time_count', 'prev_loan_number', 'interest_amount', 'loannumber', 'prev_max_loan_amount', 'interest_rate',\n",
        "             'loanamount', 'approval_hour', 'approval_speed_hours']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "648eb5ac-20ce-468b-a93e-2b4c69b8874a",
      "metadata": {
        "id": "648eb5ac-20ce-468b-a93e-2b4c69b8874a"
      },
      "outputs": [],
      "source": [
        "# After dropping columns and rechecking the vif value, the above columns were chosen to be dropped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e654a6-d153-4338-a1b9-9971aa72b130",
      "metadata": {
        "id": "e2e654a6-d153-4338-a1b9-9971aa72b130"
      },
      "outputs": [],
      "source": [
        "calculate_vif(X_train_1_vred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2febbd6b-37e2-425c-8fe9-497cbea768e1",
      "metadata": {
        "id": "2febbd6b-37e2-425c-8fe9-497cbea768e1"
      },
      "outputs": [],
      "source": [
        "#create copies\n",
        "X_train_1, X_test_1, y_train_1, y_test_1 = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea5e49e-fcdf-4ff2-a4e6-47303b2dfb6f",
      "metadata": {
        "id": "3ea5e49e-fcdf-4ff2-a4e6-47303b2dfb6f"
      },
      "outputs": [],
      "source": [
        "X_train_1_red = X_train_1.drop(cols_drop, axis=1)\n",
        "X_test_1_red = X_test_1.drop(cols_drop, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7277db-8c8a-4e16-bb2e-db40caa87cbf",
      "metadata": {
        "id": "7d7277db-8c8a-4e16-bb2e-db40caa87cbf"
      },
      "outputs": [],
      "source": [
        "def predict_evaluate_adn(X_train, X_test, y_train, y_test, model):\n",
        "    num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    # preprocessor\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', num_transformer, num_cols),\n",
        "        ('cat', cat_transformer, cat_cols),\n",
        "\n",
        "    ])\n",
        "\n",
        "    # preprocess\n",
        "    X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    # resample\n",
        "    X_train_adn, y_train_adn = adasyn.fit_resample(X_train_preprocessed, y_train)\n",
        "\n",
        "    #fit\n",
        "    model.fit(X_train_adn, y_train_adn)\n",
        "\n",
        "    # predict\n",
        "    train_pred = model.predict(X_train_adn)\n",
        "    test_pred = model.predict(X_test_processed)\n",
        "\n",
        "    #accuracy\n",
        "    train_acc = accuracy_score(y_train_adn, train_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\")\n",
        "    print(classification_report(y_test, test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49957483-d4b9-4d3d-83f6-d6b2033ec497",
      "metadata": {
        "id": "49957483-d4b9-4d3d-83f6-d6b2033ec497"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(random_state=20, max_iter=1000, solver='liblinear')\n",
        "predict_evaluate_adn(X_train_1_red, X_test_1_red, y_train_1, y_test_1, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66a60944-a069-463b-aa1d-8ae815362d99",
      "metadata": {
        "id": "66a60944-a069-463b-aa1d-8ae815362d99"
      },
      "source": [
        "#### Mutual Information\n",
        "Pick top features based on their relevance using mutual information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e012b0-5f4f-43cc-9716-e16fdee8250e",
      "metadata": {
        "id": "93e012b0-5f4f-43cc-9716-e16fdee8250e"
      },
      "outputs": [],
      "source": [
        "X_train_1_red = mini_preprocess(X_train_1_red)\n",
        "X_adn, y_adn = adasyn.fit_resample(X_train_1_red, y_train_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b4bdd5-befd-4061-b2ee-ac8d6fb3cc35",
      "metadata": {
        "id": "75b4bdd5-befd-4061-b2ee-ac8d6fb3cc35"
      },
      "outputs": [],
      "source": [
        "mutual_info = mutual_info_classif(X_adn, y_adn, random_state=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abbaed1-45ce-46d7-bd11-ac0b712c5206",
      "metadata": {
        "id": "3abbaed1-45ce-46d7-bd11-ac0b712c5206"
      },
      "outputs": [],
      "source": [
        "mutual_info = pd.Series(mutual_info).sort_values(ascending=False)\n",
        "mutual_info.index = X_train_1_red.columns\n",
        "mutual_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ba2cd4c-4ae5-4e41-9eee-f5f53fec1724",
      "metadata": {
        "id": "9ba2cd4c-4ae5-4e41-9eee-f5f53fec1724"
      },
      "outputs": [],
      "source": [
        "k = 10  # pick top 10\n",
        "top_k_features = mutual_info.head(k).index\n",
        "print(top_k_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691d6a13-bdc2-4165-bfdf-f11bd9dcf226",
      "metadata": {
        "id": "691d6a13-bdc2-4165-bfdf-f11bd9dcf226"
      },
      "outputs": [],
      "source": [
        "cols = ['termdays', 'bank_account_type', 'bank_name_clients', 'employment_status_clients', 'default_count', 'default_rate',\n",
        "        'prev_total_loan', 'approval_day', 'loan_amount_category', 'loan_number_category']\n",
        "X_train_mi = X_train_1.loc[:, cols]\n",
        "X_test_mi = X_test_1.loc[:, cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aba5a95-b42f-4d9b-967d-d5a2e574e623",
      "metadata": {
        "id": "5aba5a95-b42f-4d9b-967d-d5a2e574e623"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression(random_state=20, max_iter=1000, solver='liblinear')\n",
        "predict_evaluate_adn(X_train_mi, X_test_mi, y_train, y_test, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a333e8-8f1d-45be-8ef4-279021847ec0",
      "metadata": {
        "id": "a9a333e8-8f1d-45be-8ef4-279021847ec0"
      },
      "source": [
        "#### **Feature Selection Approach:**\n",
        "I tested two feature selection methods across different machine learning models to find the best approach for detecting loan defaults, with final selection validated against business insights to ensure practical relevance in loan default prediction.\n",
        "\n",
        "1. First Test: Using only VIF to remove related features improved logistic regression recall from 11% to 50%. But when I tried this same feature set on other models like SVC and random forest, they performed poorly.\n",
        "2. Second Test: Using mutual information + VIF gave logistic regression 42% recall at first - lower than the 50%. However, this feature set worked much better across different models, enabling fair comparison of performance.\n",
        "\n",
        "**Why I Chose Mutual Information:**\n",
        "\n",
        "Even though it reduced logistic regression performance, this method allowed me to test different models on equal footing and identify the best model for this problem. Testing multiple models with consistent features provided a more comprehensive approach than optimizing for a single model.\n",
        "\n",
        "**Alternative Approach:**\n",
        "\n",
        "I could have tuned the logistic regression with VIF features to try improving the 50% recall, but using mutual information features created a better foundation for systematic model comparison and selection.\n",
        "\n",
        "Through feature selection using VIF and the mutual information, I have been able to improve the base model's recall from 11% to 42%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239be4ac-8f20-461a-951a-a0a26e350700",
      "metadata": {
        "id": "239be4ac-8f20-461a-951a-a0a26e350700"
      },
      "source": [
        "## Training, Predicting with and Evaluating Multiple Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "200bb3f0-8849-425d-b067-e2acba06a016",
      "metadata": {
        "id": "200bb3f0-8849-425d-b067-e2acba06a016"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(solver='liblinear', max_iter=1000, random_state=20),\n",
        "    'Decision Tree' : DecisionTreeClassifier(random_state=20),\n",
        "    'Random Forest': RandomForestClassifier(random_state=20),\n",
        "    'Gradient Boost' : GradientBoostingClassifier(random_state=20),\n",
        "    'XG Boost' : XGBClassifier(random_state=20),\n",
        "    'SVC' : SVC(random_state=20)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f73d5b-ae5a-459f-b09b-69b1d7d3b677",
      "metadata": {
        "id": "63f73d5b-ae5a-459f-b09b-69b1d7d3b677"
      },
      "outputs": [],
      "source": [
        "# prediction function\n",
        "def predict_evaluate_adn_models(X_train, X_test, y_train, y_test, model_name, model):\n",
        "    num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    # preprocessor\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', num_transformer, num_cols),\n",
        "        ('cat', cat_transformer, cat_cols),\n",
        "\n",
        "    ])\n",
        "\n",
        "    # preprocess\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    # resample\n",
        "    X_train_adn, y_train_adn = adasyn.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "    #fit\n",
        "    model.fit(X_train_adn, y_train_adn)\n",
        "\n",
        "    # predict\n",
        "    train_pred = model.predict(X_train_adn)\n",
        "    test_pred = model.predict(X_test_processed)\n",
        "\n",
        "    #accuracy\n",
        "    train_acc = accuracy_score(y_train_adn, train_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "    # confusion matrix\n",
        "    train_cm = confusion_matrix(y_train_adn, train_pred)\n",
        "    test_cm = confusion_matrix(y_test, test_pred)\n",
        "\n",
        "    return {\n",
        "        'train_acc': train_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'train_cm': train_cm,\n",
        "        'test_cm': test_cm,\n",
        "        'classification_report': classification_report(y_test, test_pred, output_dict=True)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5085078-4d8c-4e5f-b903-4b5244ad03e2",
      "metadata": {
        "id": "f5085078-4d8c-4e5f-b903-4b5244ad03e2"
      },
      "outputs": [],
      "source": [
        "# predict and evaluate models\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    results[name] = predict_evaluate_adn_models(X_train_mi, X_test_mi, y_train, y_test, name, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2605c9aa-aa66-4dd4-bb9c-e6a2bf8a7e48",
      "metadata": {
        "id": "2605c9aa-aa66-4dd4-bb9c-e6a2bf8a7e48"
      },
      "outputs": [],
      "source": [
        "fig, (axes1, axes2) = plt.subplots(2, 6, figsize=(20, 4), constrained_layout=True)\n",
        "\n",
        "for (i, (name, result)), ax1, ax2 in zip(enumerate(results.items()), axes1, axes2):\n",
        "    train_cm = result['train_cm']\n",
        "    test_cm = result['test_cm']\n",
        "\n",
        "    #plot confusion matrix\n",
        "    disp_train = ConfusionMatrixDisplay(train_cm)\n",
        "    disp_test = ConfusionMatrixDisplay(test_cm)\n",
        "\n",
        "    #train cm\n",
        "    disp_train.plot(ax=ax1, cmap='Blues')\n",
        "    ax1.set_title(f'{name} Train Confusion Matrix')\n",
        "\n",
        "    #test cm\n",
        "    disp_test.plot(ax=ax2, cmap='Blues')\n",
        "    ax2.set_title(f'{name} Test Confusion Matrix')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f165f8-1fda-4902-8211-ffd2f7b20b06",
      "metadata": {
        "id": "22f165f8-1fda-4902-8211-ffd2f7b20b06"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results).T\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119a8a1e-0797-4812-96c3-156a58f00e5c",
      "metadata": {
        "id": "119a8a1e-0797-4812-96c3-156a58f00e5c"
      },
      "outputs": [],
      "source": [
        "report_dfs = {}\n",
        "\n",
        "for model, report in results_df['classification_report'].items():\n",
        "    df = pd.DataFrame(report).T\n",
        "    report_dfs[model] = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a07d4a-a6e3-4233-999a-cab398f788ac",
      "metadata": {
        "id": "b3a07d4a-a6e3-4233-999a-cab398f788ac"
      },
      "outputs": [],
      "source": [
        "all_reports = pd.concat(report_dfs, axis=0)\n",
        "display(all_reports)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9de544c1-03fe-4c74-b63a-e6b2b8194778",
      "metadata": {
        "id": "9de544c1-03fe-4c74-b63a-e6b2b8194778"
      },
      "source": [
        "Based on the recall and precision trade off, the top model is SVC which has 35% precision and 41% recall on the 0 class (bad flags).\n",
        "\n",
        "Train accuracy, Test accuracy = (0.720, 0.708)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deefb029-7000-465a-ac25-a0a53477cfc6",
      "metadata": {
        "id": "deefb029-7000-465a-ac25-a0a53477cfc6"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wXfbWNctE5Og",
      "metadata": {
        "id": "wXfbWNctE5Og"
      },
      "outputs": [],
      "source": [
        "model = SVC(random_state=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc72238c-5c82-46af-bd0e-e0610755a76f",
      "metadata": {
        "id": "bc72238c-5c82-46af-bd0e-e0610755a76f"
      },
      "outputs": [],
      "source": [
        "# using grid search csv\n",
        "\n",
        "#define parameters\n",
        "parameters= {\n",
        "    'C': [0.1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'class_weight': ['balanced', None]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "udNcjKKiFAJB",
      "metadata": {
        "id": "udNcjKKiFAJB"
      },
      "outputs": [],
      "source": [
        "# create a function for tuning\n",
        "def tune_model(X_train, X_test, y_train, y_test, model, parameters):\n",
        "    num_cols = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "    cat_cols = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "    # preprocessor\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', num_transformer, num_cols),\n",
        "        ('cat', cat_transformer, cat_cols),\n",
        "\n",
        "    ])\n",
        "\n",
        "    # preprocess\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # search\n",
        "    search = GridSearchCV(estimator=model, param_grid=parameters, n_jobs=-1, cv=5, scoring=make_scorer(recall_score, pos_label=0))\n",
        "    search.fit(X_train_processed, y_train)\n",
        "\n",
        "    best_model = search.best_estimator_\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zMr4EYDhFBl1",
      "metadata": {
        "id": "zMr4EYDhFBl1"
      },
      "outputs": [],
      "source": [
        "#tune_model(X_train_mi, X_test_mi, y_train, y_test, model, parameters)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = SVC(C=0.1, class_weight='balanced', gamma=1, kernel='sigmoid', random_state=20, probability=True)"
      ],
      "metadata": {
        "id": "lK3M-OQCUHn3"
      },
      "id": "lK3M-OQCUHn3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, predict and evaluate\n",
        "\n",
        "num_cols = X_train_mi.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = X_train_mi.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# preprocessor\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', num_transformer, num_cols),\n",
        "    ('cat', cat_transformer, cat_cols),\n",
        "\n",
        "])\n",
        "\n",
        "# preprocess\n",
        "X_train_mi_processed = preprocessor.fit_transform(X_train_mi)\n",
        "X_test_mi_processed = preprocessor.transform(X_test_mi)\n",
        "# resample\n",
        "X_train_adn, y_train_adn = adasyn.fit_resample(X_train_mi_processed, y_train)\n",
        "\n",
        "#fit\n",
        "final_model.fit(X_train_adn, y_train_adn)\n",
        "\n",
        "# predict\n",
        "train_pred = final_model.predict(X_train_adn)\n",
        "test_pred = final_model.predict(X_test_mi_processed)\n",
        "\n",
        "#accuracy\n",
        "train_acc = accuracy_score(y_train_adn, train_pred)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\")\n",
        "print(classification_report(y_test, test_pred))"
      ],
      "metadata": {
        "id": "EIIbHWsENLmh"
      },
      "id": "EIIbHWsENLmh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAP"
      ],
      "metadata": {
        "id": "AFQxi2zBQaQ1"
      },
      "id": "AFQxi2zBQaQ1"
    },
    {
      "cell_type": "code",
      "source": [
        "# get train and test samples\n",
        "background_info = X_train_mi_processed[:50]\n",
        "test_sample = X_test_mi_processed[:20]\n",
        "\n",
        "# get feature names for interpretability\n",
        "feature_names = (num_cols +\n",
        "                list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(cat_cols)))\n",
        "\n",
        "# get shap values\n",
        "explainer = shap.KernelExplainer(final_model.predict_proba, background_info)\n",
        "shap_values = explainer.shap_values(test_sample)\n",
        "\n",
        "# check shape to ensure compatibility\n",
        "print(background_info.shape, test_sample.shape, len(feature_names), shap_values[0].shape)"
      ],
      "metadata": {
        "id": "jMDgyhI9NNFw"
      },
      "id": "jMDgyhI9NNFw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correct shap values shape\n",
        "shap_values_corrected = [np.array([shap_values[i][:, 0] for i in range(20)]), np.array([shap_values[i][:, 1] for i in range(20)])]"
      ],
      "metadata": {
        "id": "wQYRVch2NUiO"
      },
      "id": "wQYRVch2NUiO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary bar plot\n",
        "shap.summary_plot(shap_values_corrected[0], test_sample, feature_names=feature_names, plot_type=\"bar\")"
      ],
      "metadata": {
        "id": "k2rgJij7NV91"
      },
      "id": "k2rgJij7NV91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the least contributing feature - approval day\n",
        "X_train_shap = X_train_mi.drop('approval_day', axis=1)\n",
        "X_test_shap = X_test_mi.drop('approval_day', axis=1)"
      ],
      "metadata": {
        "id": "WdoGgHMZNqxb"
      },
      "id": "WdoGgHMZNqxb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, predict and evaluate\n",
        "\n",
        "num_cols = X_train_shap.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_cols = X_train_shap.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# preprocessor\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', num_transformer, num_cols),\n",
        "    ('cat', cat_transformer, cat_cols),\n",
        "\n",
        "])\n",
        "\n",
        "# preprocess\n",
        "X_train_shap_processed = preprocessor.fit_transform(X_train_shap)\n",
        "X_test_shap_processed = preprocessor.transform(X_test_shap)\n",
        "# resample\n",
        "X_train_adn, y_train_adn = adasyn.fit_resample(X_train_shap_processed, y_train)\n",
        "\n",
        "#fit\n",
        "final_model.fit(X_train_adn, y_train_adn)\n",
        "\n",
        "# predict\n",
        "train_pred = final_model.predict(X_train_adn)\n",
        "test_pred = final_model.predict(X_test_shap_processed)\n",
        "\n",
        "#accuracy\n",
        "train_acc = accuracy_score(y_train_adn, train_pred)\n",
        "test_acc = accuracy_score(y_test, test_pred)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\")\n",
        "print(classification_report(y_test, test_pred))"
      ],
      "metadata": {
        "id": "JYxbF28cNsIB"
      },
      "id": "JYxbF28cNsIB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix\n",
        "test_cm = confusion_matrix(y_test, test_pred)\n",
        "disp_test = ConfusionMatrixDisplay(test_cm)\n",
        "disp_test.plot(cmap='Blues')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VHt_F-vBNtvr"
      },
      "id": "VHt_F-vBNtvr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix shows the model correctly identified 43 bad loans and 158 good loans. However, it misclassified 33 bad borrowers as good and 115 good borrowers as bad."
      ],
      "metadata": {
        "id": "ijKfnV0kN20y"
      },
      "id": "ijKfnV0kN20y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Predictive Model Analysis - Conclusion**\n",
        "\n",
        "#### **Model Development and Performance**\n",
        "\n",
        "I developed an SVC (Support Vector Classifier) to predict loan defaults using a feature selection approach that combined VIF and mutual information analysis with business insights from previous loan analysis. This ensured the model captures both statistical significance and real-world lending risk patterns.\n",
        "\n",
        "**Final Model Performance:**\n",
        "- Precision for Bad Loans: 27%\n",
        "- Recall for Bad Loans: 57%\n",
        "- F1-Score for Bad Loans: 37%\n",
        "- Overall Test Accuracy: 58%\n",
        "\n",
        "The model successfully identifies 57% of actual defaults, meaning it catches more than half of risky loans before they occurred. While precision is low at 27%, this trade off is acceptable in lending situations where missing a default (false negative) is typically more costly than declining a good applicant (false positive).\n",
        "\n",
        "#### **Feature Selection and Model Refinement**\n",
        "\n",
        "I used a hybrid approach combining statistical methods (VIF and mutual information) with business insights to select features. SHAP analysis further validated this approach by identifying `approval_day` as the least contributing feature. Removing this feature improved recall from 55% to 57% while maintaining precision, resulting in a final model with 9 highly relevant features focused on borrower behavior, demographics, and loan characteristics.\n",
        "\n",
        "#### **Dataset Limitations and Model Constraints**\n",
        "\n",
        "With 4,368 rows and a 78% vs 22% class imbalance, the dataset presents several challenges:\n",
        "- Limited Sample Size: Insufficient for highly complex models or deep feature engineering\n",
        "- Class Imbalance: I addressed this through ADASYN resampling, balanced class weights, and recall-focused optimization\n",
        "- Generalization Concerns: Model performance may vary with new data due to sample size constraints\n",
        "\n",
        "#### **Business Impact and Recommendations**\n",
        "**Immediate Value:**\n",
        "- Risk Reduction: Successfully identifying 57% of defaults represents significant potential savings for the lender.\n",
        "- Automated Screening: The model can flag high-risk applications for manual review\n",
        "- Decision Support: Insights support informed lending decisions\n",
        "\n",
        "**Implementation Recommendations:**\n",
        "- Pilot Deployment: Test the model on new applications under human supervision.\n",
        "- Threshold Adjustment: Adjust prediction thresholds to capture more defaults if the business can tolerate a higher false positive rate.\n",
        "- Continuous Monitoring: Track model performance on new data and retrain as needed\n",
        "- Data Expansion: Collect additional borrower and loan data to improve model robustness and accuracy.\n",
        "\n",
        "#### **Future Enhancements**\n",
        "1. Larger Dataset: Expand to 10,000+ samples for improved statistical reliability\n",
        "2. Advanced Models: Explore ensemble methods combining SVC with tree-based models or or other state-of-the-art algorithms.\n",
        "3. Feature Engineering: Develop richer behavioral and temporal features for better predictive power.\n",
        "4. Real-time Integration: Implement systems for live predictions and real-time decision support.\n",
        "\n",
        "#### **Final Assessment**\n",
        "\n",
        "While constrained by dataset size, I successfully built a loan default prediction model that achieves meaningful business value. The 57% recall rate for default identification, combined with SHAP-validated feature selection, provides a strong foundation for operational implementation and future model enhancement."
      ],
      "metadata": {
        "id": "mjrRwosJN4mx"
      },
      "id": "mjrRwosJN4mx"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}